{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from panda.utils.plot_utils import apply_custom_style\n",
    "\n",
    "apply_custom_style(\"../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_COLORS = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Saved Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_metric_fnames(save_dir):\n",
    "    fnames = [f for f in os.listdir(save_dir) if f.endswith(\".json\") and \"distributional_metrics\" and \"all\" in f]\n",
    "\n",
    "    def extract_window(fname):\n",
    "        m = re.search(r\"window-(\\d+)\", fname)\n",
    "        return int(m.group(1)) if m else float(\"inf\")\n",
    "\n",
    "    return sorted(fnames, key=extract_window)\n",
    "\n",
    "\n",
    "panda_metrics_save_dir = f\"{WORK_DIR}/eval_results/panda/pft_chattn_emb_w_poly-0/test_zeroshot\"\n",
    "# NOTE: we also have for chronos_nondeterministic, replace \"chronos\" with \"chronos_nondeterministic\" in the paths below\n",
    "chronos_sft_metrics_save_dir = f\"{WORK_DIR}/eval_results/chronos_nondeterministic/chronos_t5_mini_ft-0/test_zeroshot\"\n",
    "chronos_zs_metrics_save_dir = f\"{WORK_DIR}/eval_results/chronos_nondeterministic/chronos_mini_zeroshot/test_zeroshot\"\n",
    "\n",
    "panda_metrics_fnames = get_sorted_metric_fnames(panda_metrics_save_dir)\n",
    "chronos_sft_metrics_fnames = get_sorted_metric_fnames(chronos_sft_metrics_save_dir)\n",
    "chronos_zs_metrics_fnames = get_sorted_metric_fnames(chronos_zs_metrics_save_dir)\n",
    "\n",
    "print(f\"Found {len(panda_metrics_fnames)} panda metrics files: {panda_metrics_fnames}\")\n",
    "print(f\"Found {len(chronos_sft_metrics_fnames)} chronos sft metrics files: {chronos_sft_metrics_fnames}\")\n",
    "print(f\"Found {len(chronos_zs_metrics_fnames)} chronos zs metrics files: {chronos_zs_metrics_fnames}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For accumulating values across all files, for both panda and chronos_sft metrics\n",
    "\n",
    "\n",
    "def filter_none(values):\n",
    "    \"\"\"Remove None values from a list.\"\"\"\n",
    "    return [v for v in values if v is not None]\n",
    "\n",
    "\n",
    "def accumulate_metrics(metrics_fnames, metrics_save_dir):\n",
    "    avg_hellinger_accum = {\n",
    "        \"pred\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"pred_with_context\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"full\": defaultdict(lambda: defaultdict(list)),\n",
    "    }\n",
    "    kld_accum = {\n",
    "        \"pred\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"pred_with_context\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"full\": defaultdict(lambda: defaultdict(list)),\n",
    "    }\n",
    "    gpdim_accum = {\n",
    "        \"gt\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"pred\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"gt_with_context\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"pred_with_context\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"full\": defaultdict(lambda: defaultdict(list)),\n",
    "    }\n",
    "    max_lyap_accum = {\n",
    "        \"gt\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"pred\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"gt_with_context\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"pred_with_context\": defaultdict(lambda: defaultdict(list)),\n",
    "        \"full\": defaultdict(lambda: defaultdict(list)),\n",
    "    }\n",
    "    prediction_time_accum = defaultdict(list)\n",
    "\n",
    "    for fname in metrics_fnames:\n",
    "        with open(os.path.join(metrics_save_dir, fname), \"rb\") as f:\n",
    "            metrics = json.load(f)\n",
    "        n_pred_intervals = len(metrics)\n",
    "        print(f\"number of prediction intervals in {fname}: {n_pred_intervals}\")\n",
    "        for pred_interval in metrics:\n",
    "            print(pred_interval)\n",
    "            data = metrics[pred_interval]\n",
    "            for system_name, system_entry in tqdm(data, desc=f\"Processing {pred_interval}\"):\n",
    "                avg_hellinger_accum[\"pred\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"prediction_horizon\"][\"avg_hellinger_distance\"]\n",
    "                )\n",
    "                avg_hellinger_accum[\"pred_with_context\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"full_trajectory\"][\"avg_hellinger_distance\"]\n",
    "                )\n",
    "                kld_accum[\"full\"][pred_interval][system_name].append(system_entry[\"full_trajectory\"][\"kl_divergence\"])\n",
    "                max_lyap_accum[\"gt\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"prediction_horizon\"][\"max_lyap_gt\"]\n",
    "                )\n",
    "                max_lyap_accum[\"pred\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"prediction_horizon\"][\"max_lyap_pred\"]\n",
    "                )\n",
    "                max_lyap_accum[\"full\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"full_trajectory\"][\"max_lyap_full_traj\"]\n",
    "                )\n",
    "                max_lyap_accum[\"gt_with_context\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"pred_with_context\"][\"max_lyap_gt_with_context\"]\n",
    "                )\n",
    "                max_lyap_accum[\"pred_with_context\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"pred_with_context\"][\"max_lyap_pred_with_context\"]\n",
    "                )\n",
    "                gpdim_accum[\"gt\"][pred_interval][system_name].append(system_entry[\"prediction_horizon\"][\"gpdim_gt\"])\n",
    "                gpdim_accum[\"pred\"][pred_interval][system_name].append(system_entry[\"prediction_horizon\"][\"gpdim_pred\"])\n",
    "                gpdim_accum[\"gt_with_context\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"pred_with_context\"][\"gpdim_gt_with_context\"]\n",
    "                )\n",
    "                gpdim_accum[\"pred_with_context\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"pred_with_context\"][\"gpdim_pred_with_context\"]\n",
    "                )\n",
    "                gpdim_accum[\"full\"][pred_interval][system_name].append(\n",
    "                    system_entry[\"full_trajectory\"][\"gpdim_full_traj\"]\n",
    "                )\n",
    "                pred_time = system_entry[\"prediction_time\"]\n",
    "                prediction_time_accum[system_name].append(pred_time)\n",
    "\n",
    "    # Now, take the mean across all files for each metric, skipping None values\n",
    "    avg_hellinger = {k: defaultdict(dict) for k in avg_hellinger_accum.keys()}\n",
    "    kld = {k: defaultdict(dict) for k in kld_accum.keys()}\n",
    "    max_lyap = {k: defaultdict(dict) for k in max_lyap_accum.keys()}\n",
    "    gpdim = {k: defaultdict(dict) for k in gpdim_accum.keys()}\n",
    "    prediction_time = {}\n",
    "\n",
    "    for key in [\"pred\", \"pred_with_context\", \"full\"]:\n",
    "        for metric_accum, metric in [\n",
    "            (avg_hellinger_accum, avg_hellinger),\n",
    "            (kld_accum, kld),\n",
    "        ]:\n",
    "            for pred_interval in metric_accum[key]:\n",
    "                for system_name, values in metric_accum[key][pred_interval].items():\n",
    "                    filtered = filter_none(values)\n",
    "                    metric[key][pred_interval][system_name] = float(np.mean(filtered)) if filtered else None\n",
    "\n",
    "    for key in [\"gt\", \"gt_with_context\", \"pred\", \"pred_with_context\", \"full\"]:\n",
    "        for metric_accum, metric in [(gpdim_accum, gpdim), (max_lyap_accum, max_lyap)]:\n",
    "            for pred_interval in metric_accum[key]:\n",
    "                for system_name, values in metric_accum[key][pred_interval].items():\n",
    "                    filtered = filter_none(values)\n",
    "                    metric[key][pred_interval][system_name] = float(np.mean(filtered)) if filtered else None\n",
    "\n",
    "    for system_name, times in prediction_time_accum.items():\n",
    "        times_arr = np.array(filter_none(times))\n",
    "        prediction_time[system_name] = np.mean(times_arr) if len(times_arr) > 0 else None\n",
    "\n",
    "    return {\n",
    "        \"avg_hellinger\": avg_hellinger,\n",
    "        \"kld\": kld,\n",
    "        \"max_lyap\": max_lyap,\n",
    "        \"gpdim\": gpdim,\n",
    "        \"prediction_time\": prediction_time,\n",
    "    }\n",
    "\n",
    "\n",
    "# Accumulate metrics for both panda and chronos_sft\n",
    "print(\"Accumulating panda metrics...\")\n",
    "panda_metrics = accumulate_metrics(panda_metrics_fnames, panda_metrics_save_dir)\n",
    "print(\"Accumulating chronos_sft metrics...\")\n",
    "chronos_sft_metrics = accumulate_metrics(chronos_sft_metrics_fnames, chronos_sft_metrics_save_dir)\n",
    "print(\"Accumulating chronos_zs metrics...\")\n",
    "chronos_zs_metrics = accumulate_metrics(chronos_zs_metrics_fnames, chronos_zs_metrics_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panda_metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panda_metrics[\"max_lyap\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"panda\", \"chronos_sft\", \"chronos_zs\"]\n",
    "metrics = {\n",
    "    m: {k: eval(f\"{m}_metrics\")[k] for k in [\"avg_hellinger\", \"kld\", \"gpdim\", \"max_lyap\", \"prediction_time\"]}\n",
    "    for m in models\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the reason for this is for some reason the first prediction takes lot more time, prob spinning up overhead\n",
    "first_system = list(metrics[\"panda\"][\"prediction_time\"].keys())[0]\n",
    "metrics[\"panda\"][\"prediction_time\"].pop(first_system)\n",
    "metrics[\"chronos_sft\"][\"prediction_time\"].pop(first_system)\n",
    "metrics[\"chronos_zs\"][\"prediction_time\"].pop(first_system)\n",
    "print(metrics[\"panda\"][\"prediction_time\"])\n",
    "print(metrics[\"chronos_sft\"][\"prediction_time\"])\n",
    "print(metrics[\"chronos_zs\"][\"prediction_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction time mean and std for both panda and chronos_sft\n",
    "\n",
    "for model_name in [\"panda\", \"chronos_sft\", \"chronos_zs\"]:\n",
    "    prediction_times = list(metrics[model_name][\"prediction_time\"].values())\n",
    "    prediction_time_mean = np.mean(prediction_times)\n",
    "    prediction_time_std = np.std(prediction_times)\n",
    "    print(f\"{model_name} prediction time mean:\", prediction_time_mean)\n",
    "    print(f\"{model_name} prediction time std:\", prediction_time_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Lyapunov Exponent Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[\"panda\"][\"max_lyap\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the prediction interval (pred_length) of 512\n",
    "pred_length = \"128\"\n",
    "model_type = \"panda\"\n",
    "use_full_traj_gt = True\n",
    "use_context_with_preds = True\n",
    "show_figure = True\n",
    "\n",
    "model_type_title = model_type.replace(\"_\", \" \").title()\n",
    "if model_type == \"chronos_zs\":\n",
    "    model_type_title = \"Chronos\"\n",
    "elif model_type == \"chronos_sft\":\n",
    "    model_type_title = \"Chronos SFT\"\n",
    "elif model_type == \"panda\":\n",
    "    model_type_title = \"Panda\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "\n",
    "if use_full_traj_gt:\n",
    "    gt_key = \"full\"\n",
    "    gt_key_name = \"Full Trajectory\"\n",
    "else:\n",
    "    gt_key = \"gt\"\n",
    "    gt_key_name = \"Ground Truth\"\n",
    "\n",
    "if use_context_with_preds:\n",
    "    pred_key = \"pred_with_context\"\n",
    "    pred_key_name = \"Context + Prediction\"\n",
    "else:\n",
    "    pred_key = \"pred\"\n",
    "    pred_key_name = \"Prediction\"\n",
    "\n",
    "# Get the dictionaries for gtcontext and predcontext at pred_length 512 for model_type\n",
    "gt_dict = metrics[model_type][\"max_lyap\"][gt_key].get(pred_length, {})\n",
    "pred_dict = metrics[model_type][\"max_lyap\"][pred_key].get(pred_length, {})\n",
    "\n",
    "# Find the intersection of system names present in both\n",
    "system_names = set(gt_dict.keys()) & set(pred_dict.keys())\n",
    "\n",
    "# Prepare x and y data for scatter plot\n",
    "x_raw = [gt_dict[sys] for sys in system_names]\n",
    "y_raw = [pred_dict[sys] for sys in system_names]\n",
    "\n",
    "# Filter out pairs where either value is nan or inf\n",
    "x = []\n",
    "y = []\n",
    "num_invalid = 0\n",
    "for xi, yi in zip(x_raw, y_raw):\n",
    "    if np.isfinite(xi) and np.isfinite(yi) and not np.isnan(xi) and not np.isnan(yi):\n",
    "        x.append(xi)\n",
    "        y.append(yi)\n",
    "    else:\n",
    "        num_invalid += 1\n",
    "\n",
    "# Compute R^2 score\n",
    "if len(x) > 0 and len(y) > 0:\n",
    "    r2 = r2_score(x, y)\n",
    "else:\n",
    "    r2 = float(\"nan\")\n",
    "\n",
    "print(f\"Filtered out {num_invalid} invalid (nan/inf) pairs from {len(x_raw)} total.\")\n",
    "\n",
    "print(f\"{model_type_title}: {pred_key_name} vs {gt_key_name} at L_pred={pred_length}, R^2={r2:.3f}\")\n",
    "\n",
    "if show_figure:\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(x, y, color=\"black\", s=5, alpha=0.1, label=None)\n",
    "    plt.xlabel(gt_key_name, fontweight=\"bold\")\n",
    "    plt.ylabel(pred_key_name, fontweight=\"bold\")\n",
    "    plt.title(\n",
    "        rf\"{model_type_title} $\\lambda_{{\\max}}$ ($L_{{\\mathrm{{pred}}}} = {pred_length}$)\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # Prepare handles and labels for legend\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    # Plot y=x line in red dashed, but do NOT add to legend yet\n",
    "    if len(x) > 0 and len(y) > 0:\n",
    "        y_eq_x_min = min(x + y)\n",
    "        y_eq_x_max = max(x + y)\n",
    "    else:\n",
    "        y_eq_x_min = 0\n",
    "        y_eq_x_max = 1\n",
    "    (h1,) = plt.plot([y_eq_x_min, y_eq_x_max], [y_eq_x_min, y_eq_x_max], \"r--\", label=r\"$y=x$\")\n",
    "\n",
    "    # Plot line of best fit as solid red line and prepare equation+R2 for legend\n",
    "    eqn_r2_label = None\n",
    "    if len(x) > 1 and len(y) > 1:\n",
    "        # Fit line: y = m*x + b\n",
    "        m, b = np.polyfit(x, y, 1)\n",
    "        x_fit = np.array([y_eq_x_min, y_eq_x_max])\n",
    "        y_fit = m * x_fit + b\n",
    "        # Format equation for label (to be shown in legend with R^2)\n",
    "        if abs(b) < 1e-10:\n",
    "            eqn_str = rf\"$y = {m:.2f}x$\"\n",
    "        else:\n",
    "            sign = \"+\" if b >= 0 else \"-\"\n",
    "            eqn_str = rf\"$y = {m:.2f}x {sign} {abs(b):.2f}$\"\n",
    "        if not (r2 != r2):  # check for nan\n",
    "            eqn_r2_label = rf\"{eqn_str}  $(R^2 = {r2:.3f})$\"\n",
    "        else:\n",
    "            eqn_r2_label = eqn_str\n",
    "        (h2,) = plt.plot(x_fit, y_fit, color=\"red\", linestyle=\"-\", linewidth=1.5, label=eqn_r2_label)\n",
    "        # Add best fit line first, then y=x line, to put y=x below in legend\n",
    "        handles.append(h2)\n",
    "        labels.append(eqn_r2_label)\n",
    "        handles.append(h1)\n",
    "        labels.append(r\"$y=x$\")\n",
    "    else:\n",
    "        # If no best fit, just add y=x\n",
    "        handles.append(h1)\n",
    "        labels.append(r\"$y=x$\")\n",
    "\n",
    "    # Show legend in lower right, showing both lines and their labels\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "    ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "    ax.ticklabel_format(style=\"sci\", axis=\"both\", scilimits=(0, 0))\n",
    "\n",
    "    if len(handles) > 0:\n",
    "        ax.legend(handles=handles, labels=labels, loc=\"lower right\", fontsize=8, frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\n",
    "    #     os.path.join(\n",
    "    #         \"../figures\",\n",
    "    #         f\"max_lyap_r_full_pred_{pred_length}_{model_type}.pdf\",\n",
    "    #     ),\n",
    "    #     bbox_inches=\"tight\",\n",
    "    # )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the prediction interval (pred_length) of 512\n",
    "pred_lengths = [\"128\", \"512\"]\n",
    "model_types = [\"panda\", \"chronos_sft\", \"chronos_zs\"]\n",
    "use_full_traj_gt = True\n",
    "use_context_with_preds = True\n",
    "\n",
    "if use_full_traj_gt:\n",
    "    gt_key = \"full\"\n",
    "    gt_key_name = \"Full Trajectory\"\n",
    "else:\n",
    "    gt_key = \"gt\"\n",
    "    gt_key_name = \"Ground Truth\"\n",
    "\n",
    "if use_context_with_preds:\n",
    "    pred_key = \"pred_with_context\"\n",
    "    pred_key_name = \"Context + Prediction\"\n",
    "else:\n",
    "    pred_key = \"pred\"\n",
    "    pred_key_name = \"Prediction\"\n",
    "\n",
    "print(f\"({pred_key_name}) vs {gt_key_name}\")\n",
    "\n",
    "for model_type in model_types:\n",
    "    model_type_title = model_type.replace(\"_\", \" \").title()\n",
    "    if model_type == \"chronos_zs\":\n",
    "        model_type_title = \"Chronos\"\n",
    "    elif model_type == \"chronos_sft\":\n",
    "        model_type_title = \"Chronos SFT\"\n",
    "    elif model_type == \"panda\":\n",
    "        model_type_title = \"Panda\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "\n",
    "    print(f\"Model Type: {model_type}\")\n",
    "\n",
    "    for pred_length in pred_lengths:\n",
    "        print(f\"Prediction Length L_pred = {pred_length}\")\n",
    "\n",
    "        # Get the dictionaries for gtcontext and predcontext at pred_length 512 for model_type\n",
    "        gt_dict = metrics[model_type][\"max_lyap\"][gt_key].get(pred_length, {})\n",
    "        pred_dict = metrics[model_type][\"max_lyap\"][pred_key].get(pred_length, {})\n",
    "\n",
    "        # Find the intersection of system names present in both\n",
    "        system_names = set(gt_dict.keys()) & set(pred_dict.keys())\n",
    "\n",
    "        # Prepare x and y data for scatter plot\n",
    "        x_raw = [gt_dict[sys] for sys in system_names]\n",
    "        y_raw = [pred_dict[sys] for sys in system_names]\n",
    "\n",
    "        # Filter out pairs where either value is nan or inf\n",
    "        x = []\n",
    "        y = []\n",
    "        num_invalid = 0\n",
    "        for xi, yi in zip(x_raw, y_raw):\n",
    "            if np.isfinite(xi) and np.isfinite(yi) and not np.isnan(xi) and not np.isnan(yi):\n",
    "                x.append(xi)\n",
    "                y.append(yi)\n",
    "            else:\n",
    "                num_invalid += 1\n",
    "\n",
    "        # Compute R^2 score\n",
    "        if len(x) > 0 and len(y) > 0:\n",
    "            r2 = r2_score(x, y)\n",
    "        else:\n",
    "            r2 = float(\"nan\")\n",
    "\n",
    "        print(f\"Filtered out {num_invalid} invalid (nan/inf) pairs from {len(x_raw)} total.\")\n",
    "\n",
    "        print(f\"R^2={r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Rosenstein Lyapunov Exponents of the full trajectory\n",
    "full_traj_lyap_r_fpath = os.path.join(WORK_DIR, \"eval_results\", \"dataset\", \"max_lyap_r_test_zeroshot.json\")\n",
    "full_traj_lyap_r_lst = json.load(open(full_traj_lyap_r_fpath))[\"4096\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_traj_lyap_r_dict = {entry[0]: entry[1][\"max_lyap_rosenstein\"] for entry in full_traj_lyap_r_lst}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_traj_lyap_r_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the prediction interval (pred_length) of 512\n",
    "pred_lengths = [\"128\", \"512\"]\n",
    "model_types = [\"panda\", \"chronos_sft\", \"chronos_zs\"]\n",
    "use_full_traj_gt = True\n",
    "use_context_with_preds = False\n",
    "\n",
    "if use_full_traj_gt:\n",
    "    gt_key = \"full\"\n",
    "    gt_key_name = \"Full Trajectory\"\n",
    "else:\n",
    "    gt_key = \"gt\"\n",
    "    gt_key_name = \"Ground Truth\"\n",
    "\n",
    "if use_context_with_preds:\n",
    "    pred_key = \"pred_with_context\"\n",
    "    pred_key_name = \"Context + Prediction\"\n",
    "else:\n",
    "    pred_key = \"pred\"\n",
    "    pred_key_name = \"Prediction\"\n",
    "\n",
    "print(f\"({pred_key_name}) vs {gt_key_name}\")\n",
    "\n",
    "for model_type in model_types:\n",
    "    model_type_title = model_type.replace(\"_\", \" \").title()\n",
    "    if model_type == \"chronos_zs\":\n",
    "        model_type_title = \"Chronos\"\n",
    "    elif model_type == \"chronos_sft\":\n",
    "        model_type_title = \"Chronos SFT\"\n",
    "    elif model_type == \"panda\":\n",
    "        model_type_title = \"Panda\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "\n",
    "    print(f\"Model Type: {model_type}\")\n",
    "\n",
    "    for pred_length in pred_lengths:\n",
    "        print(f\"Prediction Length L_pred = {pred_length}\")\n",
    "\n",
    "        # Get the dictionaries for gtcontext and predcontext at pred_length 512 for model_type\n",
    "        gt_dict = metrics[model_type][\"max_lyap\"][gt_key].get(pred_length, {})\n",
    "        # gt_dict = full_traj_lyap_r_dict\n",
    "        pred_dict = metrics[model_type][\"max_lyap\"][pred_key].get(pred_length, {})\n",
    "\n",
    "        # Find the intersection of system names present in both\n",
    "        system_names = set(gt_dict.keys()) & set(pred_dict.keys())\n",
    "\n",
    "        # Prepare x and y data for scatter plot\n",
    "        x_raw = [gt_dict[sys] for sys in system_names]\n",
    "        y_raw = [pred_dict[sys] for sys in system_names]\n",
    "\n",
    "        # Filter out pairs where either value is nan or inf\n",
    "        x = []\n",
    "        y = []\n",
    "        num_invalid = 0\n",
    "        for xi, yi in zip(x_raw, y_raw):\n",
    "            if np.isfinite(xi) and np.isfinite(yi) and not np.isnan(xi) and not np.isnan(yi):\n",
    "                x.append(xi)\n",
    "                y.append(yi)\n",
    "            else:\n",
    "                num_invalid += 1\n",
    "\n",
    "        # Compute R^2 score\n",
    "        # NOTE: also can swap out with pearsonr here to compute pearson correlation\n",
    "        if len(x) > 0 and len(y) > 0:\n",
    "            r2 = r2_score(x, y)\n",
    "        else:\n",
    "            r2 = float(\"nan\")\n",
    "\n",
    "        # print(\n",
    "        #     f\"Filtered out {num_invalid} invalid (nan/inf) pairs from {len(x_raw)} total.\"\n",
    "        # )\n",
    "\n",
    "        print(f\"R^2={r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[\"chronos_sft\"][\"max_lyap\"][\"pred\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "full_lyaps = [full_traj_lyap_r_dict[sys] for sys in full_traj_lyap_r_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"gt key: {gt_key}\")\n",
    "print(f\"pred key: {pred_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyaps_per_model = {model_name: metrics[model_name][\"max_lyap\"][pred_key] for model_name in metrics.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(dict)\n",
    "for model in lyaps_per_model.keys():\n",
    "    lyaps_for_model = lyaps_per_model[model]\n",
    "    for pred_length in lyaps_for_model.keys():\n",
    "        if pred_length == \"128\":  # TODO: remove when we have 128 predictions\n",
    "            continue\n",
    "        model_lyaps = lyaps_for_model[pred_length]\n",
    "        model_lyaps = [model_lyaps[sys] for sys in model_lyaps.keys()]\n",
    "        assert len(model_lyaps) == len(full_lyaps)\n",
    "\n",
    "        # measure correlation and wilcoxon signed rank test statistics and pvalues\n",
    "        result = pearsonr(full_lyaps, model_lyaps)\n",
    "        results[model][pred_length] = {\n",
    "            \"corr\": float(f\"{result.statistic:.3f}\"),\n",
    "            \"pval\": float(f\"{result.pvalue:.3e}\"),\n",
    "        }\n",
    "\n",
    "\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for model in lyaps_per_model.keys():\n",
    "    lyaps_for_model = lyaps_per_model[model]\n",
    "    for pred_length in lyaps_for_model.keys():\n",
    "        if pred_length == \"128\":  # TODO: remove when we have 128 predictions\n",
    "            continue\n",
    "        model_lyaps = lyaps_for_model[pred_length]\n",
    "        model_lyaps = [model_lyaps[sys] for sys in model_lyaps.keys()]\n",
    "        assert len(model_lyaps) == len(full_lyaps)\n",
    "\n",
    "        # measure R2 score\n",
    "        r2 = r2_score(full_lyaps, model_lyaps)\n",
    "        results[model][pred_length] = {\n",
    "            \"r2\": float(f\"{r2:.3f}\"),\n",
    "        }\n",
    "\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nans(values):\n",
    "    # Convert dict_values to list, filter out None, then filter out NaN\n",
    "    arr = []\n",
    "    for v in list(values):\n",
    "        if v is not None and not (isinstance(v, float) and np.isnan(v)):\n",
    "            arr.append(float(v))\n",
    "    return np.array(arr, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_histograms(\n",
    "    metric_name,\n",
    "    pred_length,\n",
    "    horizon_name,\n",
    "    model_labels,\n",
    "    model_keys,\n",
    "    bins=25,\n",
    "    log_scale=False,\n",
    "    xlabel=None,\n",
    "    ylabel=\"Count\",\n",
    "    title=None,\n",
    "    filename=None,\n",
    "    alpha_val=0.6,\n",
    "    legend_loc=\"upper right\",\n",
    "):\n",
    "    # Gather data for each model\n",
    "    metric_data = []\n",
    "    for model in model_keys:\n",
    "        values = metrics[model][metric_name][horizon_name][pred_length].values()\n",
    "        if metric_name == \"avg_hellinger\":\n",
    "            arr = filter_nans(values)\n",
    "        else:\n",
    "            arr = np.array([v for v in values if v is not None], dtype=float)\n",
    "        metric_data.append(arr)\n",
    "\n",
    "    # Concatenate all for bin calculation\n",
    "    if log_scale:\n",
    "        all_vals = np.concatenate([d[d > 0] for d in metric_data])\n",
    "        if len(all_vals) > 0:\n",
    "            min_val = np.min(all_vals)\n",
    "            max_val = np.max(all_vals)\n",
    "            bins = np.logspace(np.log10(min_val), np.log10(max_val), bins)\n",
    "        else:\n",
    "            print(\"No positive values found\")\n",
    "    else:\n",
    "        all_vals = np.concatenate(metric_data)\n",
    "        bins = np.histogram_bin_edges(all_vals, bins=bins)\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for i, (arr, label) in enumerate(zip(metric_data, model_labels)):\n",
    "        color = (\n",
    "            DEFAULT_COLORS[i]\n",
    "            if i < len(DEFAULT_COLORS)\n",
    "            else f\"tab:{['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan'][i % 10]}\"\n",
    "        )\n",
    "        edgecolor = color\n",
    "        arr_plot = arr\n",
    "        if log_scale:\n",
    "            arr_plot = arr[arr > 0]\n",
    "        plt.hist(\n",
    "            arr_plot,\n",
    "            bins=bins,\n",
    "            color=color,\n",
    "            edgecolor=edgecolor,\n",
    "            alpha=alpha_val,\n",
    "            histtype=\"stepfilled\",\n",
    "            label=label,\n",
    "        )\n",
    "    if log_scale:\n",
    "        plt.xscale(\"log\")\n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel, fontweight=\"bold\")\n",
    "    plt.ylabel(ylabel, fontweight=\"bold\")\n",
    "    plt.legend(loc=legend_loc)\n",
    "    if title:\n",
    "        plt.title(title, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    if filename:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot Hellinger Distance\n",
    "plot_metric_histograms(\n",
    "    metric_name=\"avg_hellinger\",\n",
    "    pred_length=\"512\",\n",
    "    horizon_name=\"pred_with_context\",  # between gt_with_context and pred_with_context\n",
    "    model_labels=[\"Panda\", \"Chronos 20M SFT\", \"Chronos 20M\"],\n",
    "    model_keys=[\"panda\", \"chronos_sft\", \"chronos_zs\"],\n",
    "    bins=25,\n",
    "    log_scale=False,\n",
    "    # xlabel=\"Average Hellinger\",\n",
    "    ylabel=\"Count\",\n",
    "    title=\"Avg Hellinger Distance ($L_{\\\\mathrm{pred}} = 512$)\",\n",
    "    filename=os.path.join(\n",
    "        \"../figures\",\n",
    "        \"avg_hellinger_distribution_pred_with_context_512.pdf\",\n",
    "    ),\n",
    "    alpha_val=0.6,\n",
    "    legend_loc=\"upper right\",\n",
    ")\n",
    "\n",
    "# Plot KL Divergence\n",
    "plot_metric_histograms(\n",
    "    metric_name=\"kld\",\n",
    "    pred_length=\"128\",\n",
    "    horizon_name=\"full\",  # between full_traj and predicrtions\n",
    "    model_labels=[\"Panda\", \"Chronos 20M SFT\", \"Chronos 20M\"],\n",
    "    model_keys=[\"panda\", \"chronos_sft\", \"chronos_zs\"],\n",
    "    bins=20,\n",
    "    log_scale=True,\n",
    "    # xlabel=\"Average KL Divergence\",\n",
    "    ylabel=\"Count\",\n",
    "    title=\"KL Divergence ($L_{\\\\mathrm{pred}} = 128$)\",\n",
    "    filename=os.path.join(\n",
    "        \"../figures\",\n",
    "        \"kld_distribution_pred_with_context_128_log.pdf\",\n",
    "    ),\n",
    "    alpha_val=0.6,\n",
    "    legend_loc=\"upper left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
