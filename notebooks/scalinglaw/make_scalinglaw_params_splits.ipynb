{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections.abc import Callable\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rseed = 99\n",
    "rng = np.random.default_rng(rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../../outputs\"\n",
    "output_path = os.path.join(output_dir, \"filtered_params_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_params_dicts = json.load(open(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(reloaded_params_dicts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_systems = sum([len(v) for v in reloaded_params_dicts.values()])\n",
    "print(f\"tot_systems_reloaded: {tot_systems}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scalinglaw_splits = 7\n",
    "split_sizes = [tot_systems]\n",
    "scalinglaw_syssample_indices = [np.arange(tot_systems)]\n",
    "for i in range(n_scalinglaw_splits):\n",
    "    curr_split_size = int(tot_systems // (2 ** (i + 1)))\n",
    "    split_sizes.append(curr_split_size)\n",
    "    curr_syssample_indices = rng.choice(\n",
    "        scalinglaw_syssample_indices[i], size=curr_split_size, replace=False\n",
    "    )\n",
    "    scalinglaw_syssample_indices.append(curr_syssample_indices)\n",
    "print(split_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, syssample_indices in enumerate(scalinglaw_syssample_indices):\n",
    "    print(f\"number of syssample_indices for split {i}: {syssample_indices.shape[0]}\")\n",
    "    if i > 0:\n",
    "        assert np.all(\n",
    "            np.isin(syssample_indices, scalinglaw_syssample_indices[i - 1])\n",
    "        ), \"smaller splits must be a subset of the previous split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdir_sample_counts_dict = {}\n",
    "for system_name, system_params in reloaded_params_dicts.items():\n",
    "    n_samples = len(system_params)\n",
    "    subdir_sample_counts_dict[system_name] = n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(list(subdir_sample_counts_dict.values())) == tot_systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_name_for_sample_idx(sample_idx, subdir_sample_counts_dict):\n",
    "    if sample_idx < 0 or sample_idx >= tot_systems:\n",
    "        raise ValueError(f\"sample_idx must be between 0 and {tot_systems - 1}\")\n",
    "\n",
    "    cumulative_count = 0\n",
    "    for system_name, count in subdir_sample_counts_dict.items():\n",
    "        if sample_idx < cumulative_count + count:\n",
    "            return system_name\n",
    "        cumulative_count += count\n",
    "\n",
    "    return None  # Should never reach here if sample_idx is valid\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sample_idx = 100\n",
    "system_name = get_system_name_for_sample_idx(sample_idx, subdir_sample_counts_dict)\n",
    "print(f\"Sample index {sample_idx} belongs to system: {system_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_idx_mapping(\n",
    "    subdir_sample_counts_dict: dict[str, int],\n",
    ") -> Callable[[np.ndarray | list[int]], tuple[np.ndarray, np.ndarray]]:\n",
    "    # Create arrays for fast lookup\n",
    "    system_names = []\n",
    "    boundaries = [0]  # Start with 0\n",
    "\n",
    "    # Build the boundaries and system names arrays\n",
    "    for system_name, count in subdir_sample_counts_dict.items():\n",
    "        system_names.append(system_name)\n",
    "        boundaries.append(boundaries[-1] + count)\n",
    "\n",
    "    # Convert to numpy arrays for faster operations\n",
    "    boundaries = np.array(boundaries)\n",
    "    system_names = np.array(system_names)\n",
    "\n",
    "    def get_system_names_and_positions(\n",
    "        sample_idxs: np.ndarray | list[int],\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        # Validate input\n",
    "        sample_idxs = np.asarray(sample_idxs)\n",
    "        if np.any((sample_idxs < 0) | (sample_idxs >= tot_systems)):\n",
    "            raise ValueError(f\"All sample_idxs must be between 0 and {tot_systems - 1}\")\n",
    "\n",
    "        # Find the index where each sample_idx would be inserted in boundaries\n",
    "        # Subtract 1 to get the correct system index\n",
    "        system_indices = np.searchsorted(boundaries, sample_idxs, side=\"right\") - 1\n",
    "\n",
    "        # Calculate relative positions within each system\n",
    "        relative_positions = sample_idxs - boundaries[system_indices]\n",
    "\n",
    "        # Return both the system names and relative positions\n",
    "        return system_names[system_indices], relative_positions\n",
    "\n",
    "    return get_system_names_and_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping function\n",
    "get_system_names_and_positions = create_sample_idx_mapping(subdir_sample_counts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx_lst = [0, 1, 43, 44, 200, 300]\n",
    "system_names, positions = get_system_names_and_positions(sample_idx_lst)\n",
    "for idx, name, pos in zip(sample_idx_lst, system_names, positions):\n",
    "    print(f\"Sample index {idx} belongs to system: {name} at position {pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dicts_all_splits = []\n",
    "for i, curr_syssample_indices in tqdm(\n",
    "    enumerate(scalinglaw_syssample_indices),\n",
    "    desc=\"Splitting params dicts for scalinglaw splits\",\n",
    "):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    params_dict_split = defaultdict(list)\n",
    "    # Sort the system sample indices to ensure consistent ordering\n",
    "    curr_syssample_indices = np.sort(curr_syssample_indices)\n",
    "    print(\n",
    "        f\"number of syssample_indices for split {i}: {curr_syssample_indices.shape[0]}\"\n",
    "    )\n",
    "    # validate that the current split is a subset of the previous split\n",
    "    if i > 0:\n",
    "        assert np.all(\n",
    "            np.isin(curr_syssample_indices, scalinglaw_syssample_indices[i - 1])\n",
    "        ), \"smaller splits must be a subset of the previous split\"\n",
    "\n",
    "    system_names, positions = get_system_names_and_positions(curr_syssample_indices)\n",
    "\n",
    "    for system_name, pos in zip(system_names, positions):\n",
    "        params_dict_split[system_name].append(reloaded_params_dicts[system_name][pos])\n",
    "    params_dicts_all_splits.append(params_dict_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(v) for v in params_dicts_all_splits[-1].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dicts_all_splits[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(params_dicts_all_splits[-6][\"AtmosphericRegime_Hadley\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(params_dicts_all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to lists for JSON serialization\n",
    "def convert_numpy_to_list(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_numpy_to_list(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_to_list(item) for item in obj]\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, param_dict_split in enumerate(params_dicts_all_splits):\n",
    "#     curr_tot_systems = sum([len(v) for v in param_dict_split.values()])\n",
    "#     print(f\"number of systems in split: {curr_tot_systems}\")\n",
    "\n",
    "#     curr_output_path = os.path.join(\n",
    "#         output_dir, f\"params_dict_split_{curr_tot_systems}.json\"\n",
    "#     )\n",
    "#     # Create a serializable version of the dictionary\n",
    "#     serializable_params_dict = {}\n",
    "#     for system_name, system_param_dicts in param_dict_split.items():\n",
    "#         serializable_params_dict[system_name] = [\n",
    "#             convert_numpy_to_list(param_dict) for param_dict in system_param_dicts\n",
    "#         ]\n",
    "\n",
    "#     with open(curr_output_path, \"w\") as f:\n",
    "#         json.dump(serializable_params_dict, f, indent=2)\n",
    "\n",
    "#     print(f\"Saved filtered parameters to {curr_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalinglaw_syssample_indices[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(reversed(scalinglaw_syssample_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dicts_all_splits_filtered = []\n",
    "reversed_scalinglaw_syssample_indices = list(reversed(scalinglaw_syssample_indices))\n",
    "\n",
    "for i, curr_syssample_indices in tqdm(\n",
    "    enumerate(reversed_scalinglaw_syssample_indices),\n",
    "    desc=\"Splitting params dicts for scalinglaw splits\",\n",
    "):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    params_dict_split = defaultdict(list)\n",
    "    # Sort the system sample indices to ensure consistent ordering\n",
    "    curr_syssample_indices = np.sort(curr_syssample_indices)\n",
    "    print(\n",
    "        f\"number of syssample_indices for split {i}: {curr_syssample_indices.shape[0]}\"\n",
    "    )\n",
    "    prev_syssample_indices = reversed_scalinglaw_syssample_indices[i - 1]\n",
    "\n",
    "    # validate that the current split is a subset of the previous split\n",
    "    if i > 0:\n",
    "        assert np.all(np.isin(prev_syssample_indices, curr_syssample_indices)), (\n",
    "            \"smaller splits must be a subset of the previous split\"\n",
    "        )\n",
    "\n",
    "    curr_syssample_indices = np.setdiff1d(\n",
    "        curr_syssample_indices, prev_syssample_indices\n",
    "    )\n",
    "    print(\n",
    "        f\"number of syssample_indices for split {i} after filtering out subset in previous split: {curr_syssample_indices.shape[0]}\"\n",
    "    )\n",
    "\n",
    "    system_names, positions = get_system_names_and_positions(curr_syssample_indices)\n",
    "\n",
    "    for system_name, pos in zip(system_names, positions):\n",
    "        params_dict_split[system_name].append(reloaded_params_dicts[system_name][pos])\n",
    "    params_dicts_all_splits_filtered.append(params_dict_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param_dict_split in enumerate(params_dicts_all_splits_filtered):\n",
    "    curr_tot_systems = sum([len(v) for v in param_dict_split.values()])\n",
    "    prev_tot_systems = len(reversed_scalinglaw_syssample_indices[i])\n",
    "    print(f\"number of systems in split: {curr_tot_systems}\")\n",
    "    print(f\"number of systems in previous split: {prev_tot_systems}\")\n",
    "    start_idx = prev_tot_systems\n",
    "    end_idx = start_idx + curr_tot_systems\n",
    "    print(f\"start_idx: {start_idx}, end_idx: {end_idx}\")\n",
    "    curr_output_path = os.path.join(\n",
    "        output_dir, f\"params_dict_split_{start_idx}-{end_idx}.json\"\n",
    "    )\n",
    "    # Create a serializable version of the dictionary\n",
    "    serializable_params_dict = {}\n",
    "    for system_name, system_param_dicts in param_dict_split.items():\n",
    "        serializable_params_dict[system_name] = [\n",
    "            convert_numpy_to_list(param_dict) for param_dict in system_param_dicts\n",
    "        ]\n",
    "\n",
    "    print(f\"check: {sum([len(v) for v in serializable_params_dict.values()])}\")\n",
    "    with open(curr_output_path, \"w\") as f:\n",
    "        json.dump(serializable_params_dict, f, indent=2)\n",
    "\n",
    "    print(f\"Saved filtered parameters to {curr_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
