{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from panda.utils.eval_utils import get_summary_metrics_dict\n",
    "from panda.utils.plot_utils import (\n",
    "    apply_custom_style,\n",
    "    make_box_plot,\n",
    "    plot_all_metrics_by_prediction_length,\n",
    ")\n",
    "\n",
    "apply_custom_style(\"../../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_COLORS = list(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = os.path.join(\"../../figures\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"test_zeroshot\"\n",
    "\n",
    "run_metrics_dir_dict = {\n",
    "    \"Panda\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"patchtst\",\n",
    "        # \"pft_stand_rff_only_pretrained-0\",\n",
    "        # \"pft_chattn_noembed_pretrained_correct-0\",\n",
    "        \"pft_chattn_emb_w_poly-0\",\n",
    "        # \"pft_linattnpolyemb_from_scratch-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Panda Univariate\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"patchtst_univariate\",\n",
    "        # \"pft_stand_rff_only_pretrained-0\",\n",
    "        # \"pft_chattn_noembed_pretrained_correct-0\",\n",
    "        \"pft_chattn_emb_w_poly-0\",\n",
    "        # \"pft_linattnpolyemb_from_scratch-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M SFT\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        # \"chronos_nondeterministic\",\n",
    "        # \"chronos_sft\",\n",
    "        \"chronos\",\n",
    "        \"chronos_t5_mini_ft-0\",\n",
    "        # \"chronos_mini_ft-0\",\n",
    "        # \"chronos_finetune_stand_updated-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 46M SFT\": os.path.join(WORK_DIR, \"eval_results\", \"chronos\", \"chronos_small_ft_equalized-13\", data_split),\n",
    "    # \"Chronos 20M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     # \"chronos_nondeterministic\",\n",
    "    #     \"chronos\",\n",
    "    #     \"chronos_mini_zeroshot\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Time MOE 50M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"timemoe\",\n",
    "    #     \"timemoe-50m\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"TimesFM 200M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"timesfm\",\n",
    "    #     \"timesfm-200m\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    \"Chronos 200M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"chronos\",\n",
    "        # \"chronos_nondeterministic\",\n",
    "        \"chronos_base_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 200M Probabilistic\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        # \"chronos\",\n",
    "        \"chronos_nondeterministic\",\n",
    "        \"chronos_base_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "    # \"Mean\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"mean\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Fourier\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"fourier\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs = defaultdict(dict)\n",
    "for model_name, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "    if not os.path.exists(run_metrics_dir):\n",
    "        print(f\"Run metrics dir does not exist: {run_metrics_dir}\")\n",
    "        continue\n",
    "    for file in sorted(\n",
    "        filter(lambda x: x.endswith(\".csv\"), os.listdir(run_metrics_dir)),\n",
    "        key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "    ):\n",
    "        if file.endswith(\".csv\"):\n",
    "            prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "            with open(os.path.join(run_metrics_dir, file)) as f:\n",
    "                metrics = pd.read_csv(f).to_dict()\n",
    "                metrics_all_runs[model_name][prediction_length] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics_all_runs[\"Chronos 20M SFT\"][64][\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics = defaultdict(dict)\n",
    "for model_name, all_metrics_of_model in metrics_all_runs.items():\n",
    "    for prediction_length, metrics in all_metrics_of_model.items():\n",
    "        systems = metrics[\"system\"]\n",
    "        metrics_unrolled = {k: list(v.values()) for k, v in metrics.items() if k != \"system\"}\n",
    "        unrolled_metrics[model_name][prediction_length] = metrics_unrolled\n",
    "\n",
    "n_runs = len(unrolled_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "p_values = defaultdict(dict)\n",
    "for baseline, baseline_metrics_per_pred_len in unrolled_metrics.items():\n",
    "    if baseline == \"Panda\":\n",
    "        continue\n",
    "    for prediction_length, baseline_metrics in baseline_metrics_per_pred_len.items():\n",
    "        for metric, baseline_metric_values in baseline_metrics.items():\n",
    "            if metric not in [\"mae\", \"mse\", \"smape\"]:\n",
    "                continue\n",
    "            panda_metric_values = unrolled_metrics[\"Panda\"][prediction_length][metric]\n",
    "            result = wilcoxon(panda_metric_values, baseline_metric_values, correction=True)\n",
    "\n",
    "            for key in [\"pvalue\", \"statistic\"]:\n",
    "                metric_key = f\"{metric}_{key}\"\n",
    "                value = getattr(result, key)\n",
    "                if metric_key in p_values[prediction_length]:\n",
    "                    p_values[prediction_length][metric_key][baseline] = value\n",
    "                else:\n",
    "                    p_values[prediction_length][metric_key] = {baseline: value}\n",
    "\n",
    "\n",
    "pvals_128 = pd.DataFrame(p_values[128]).dropna()\n",
    "pvals_256 = pd.DataFrame(p_values[256]).dropna()\n",
    "pvals_512 = pd.DataFrame(p_values[512]).dropna()\n",
    "\n",
    "for df in [pvals_128, pvals_256, pvals_512]:\n",
    "    for col in filter(lambda x: \"pvalue\" in x, df.columns):\n",
    "        correction = multipletests(df[col])\n",
    "        df[f\"{col}_pval_adj\"] = correction[1]\n",
    "        df[f\"{col}_reject\"] = correction[0]\n",
    "\n",
    "pvals_128.to_csv(\"../outputs/pvals_128.csv\")\n",
    "pvals_256.to_csv(\"../outputs/pvals_256.csv\")\n",
    "pvals_512.to_csv(\"../outputs/pvals_512.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = DEFAULT_COLORS[: n_runs + 1]\n",
    "default_colors = default_colors[:3] + default_colors[4:]\n",
    "print(default_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"smape\"\n",
    "legend_handles = make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=selected_metric,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=default_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"{figs_save_dir}/{selected_metric}_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(25, 75),\n",
    "    whisker_percentile_range=(5, 95),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2, 4)},\n",
    "    box_width=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"mae\"\n",
    "legend_handles = make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=selected_metric,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=default_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"{figs_save_dir}/{selected_metric}_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(25, 75),\n",
    "    whisker_percentile_range=(10, 80),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2, 4)},\n",
    "    box_width=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=5,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_horizontal_patches.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=1,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_vertical_patches.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict, has_nans = get_summary_metrics_dict(unrolled_metrics, \"smape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mse\", \"mae\", \"smape\", \"spearman\"]\n",
    "metrics_dicts, has_nans = zip(*[get_summary_metrics_dict(unrolled_metrics, metric) for metric in metrics])\n",
    "all_metrics_dict = {m: metrics_dicts[i] for i, m in enumerate(metrics)}\n",
    "has_nans_dict = {m: has_nans[i] for i, m in enumerate(metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaNs for each metric and model\n",
    "nan_counts = {}\n",
    "for metric_name, metric_data in all_metrics_dict.items():\n",
    "    nan_counts[metric_name] = {}\n",
    "    for model_name, model_data in metric_data.items():\n",
    "        # all_vals = np.concatenate(model_data[\"all_vals\"])\n",
    "        all_vals_pred128 = model_data[\"all_vals\"][1]\n",
    "        nan_count = np.isnan(all_vals_pred128).sum()\n",
    "        nan_counts[metric_name][model_name] = nan_count\n",
    "        if nan_count > 0:\n",
    "            print(f\"Found {nan_count} NaNs in {model_name} for {metric_name}\")\n",
    "\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_nans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order model names by sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_ordering = []  # sorted by median smape at 128\n",
    "for model_name, data in all_metrics_dict[\"smape\"].items():\n",
    "    median_metrics_128 = data[\"medians\"][1]\n",
    "    model_names_ordering.append((model_name, median_metrics_128))\n",
    "model_names_ordering = sorted(model_names_ordering, key=lambda x: x[1])\n",
    "model_names_ordering = [x[0] for x in model_names_ordering]\n",
    "print(model_names_ordering)\n",
    "\n",
    "# Reorder all_metrics_dict according to model_names_ordering for each metric\n",
    "reordered_metrics_dict = {}\n",
    "for metric_name, metric_data in all_metrics_dict.items():\n",
    "    reordered_metric_data = {}\n",
    "\n",
    "    # Add models in the order specified by model_names_ordering\n",
    "    for model_name in model_names_ordering:\n",
    "        if model_name in metric_data:\n",
    "            reordered_metric_data[model_name] = metric_data[model_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not found in {metric_name}\")\n",
    "\n",
    "    reordered_metrics_dict[metric_name] = reordered_metric_data\n",
    "all_metrics_dict = reordered_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles = plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    metrics_to_show_envelope=[\"mae\", \"smape\"],\n",
    "    n_cols=4,\n",
    "    n_rows=1,\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_metrics_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"loc\": \"upper left\", \"frameon\": True, \"fontsize\": 10},\n",
    "    colors=default_colors,\n",
    "    use_inv_spearman=True,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=5,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_horizontal.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=1,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_vertical.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"smape\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"mae\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"mse\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    # metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"spearman\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    "    use_inv_spearman=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
