{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from panda.utils.eval_utils import get_summary_metrics_dict\n",
    "from panda.utils.plot_utils import (\n",
    "    apply_custom_style,\n",
    "    make_box_plot,\n",
    "    plot_all_metrics_by_prediction_length,\n",
    ")\n",
    "\n",
    "apply_custom_style(\"../../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_COLORS = list(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = os.path.join(\"../../figures\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)\n",
    "\n",
    "outputs_save_dir = os.path.join(\"../../outputs\", \"eval_metrics\")\n",
    "os.makedirs(outputs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"test_zeroshot\"\n",
    "\n",
    "run_metrics_dir_dict = {\n",
    "    \"Panda\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results_old\",\n",
    "        \"patchtst\",\n",
    "        # \"pft_stand_rff_only_pretrained-0\",\n",
    "        # \"pft_chattn_noembed_pretrained_correct-0\",\n",
    "        \"pft_chattn_emb_w_poly-0\",\n",
    "        # \"pft_linattnpolyemb_from_scratch-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    # \"Panda Univariate\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"patchtst_univariate\",\n",
    "    #     # \"pft_stand_rff_only_pretrained-0\",\n",
    "    #     # \"pft_chattn_noembed_pretrained_correct-0\",\n",
    "    #     \"pft_chattn_emb_w_poly-0\",\n",
    "    #     # \"pft_linattnpolyemb_from_scratch-0\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    \"Chronos 20M SFT\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results_old\",\n",
    "        \"chronos_nondeterministic\",\n",
    "        # \"chronos_sft\",\n",
    "        # \"chronos\",\n",
    "        \"chronos_t5_mini_ft-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    # \"Chronos 46M SFT\": os.path.join(WORK_DIR, \"eval_results\", \"chronos\", \"chronos_small_ft_equalized-13\", data_split),\n",
    "    # \"Chronos 20M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results_old\",\n",
    "    #     # \"chronos_nondeterministic\",\n",
    "    #     \"chronos\",\n",
    "    #     \"chronos_mini_zeroshot\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    \"Chronos 200M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results_old\",\n",
    "        # \"chronos\",\n",
    "        \"chronos_nondeterministic\",\n",
    "        \"chronos_base_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Time MOE 50M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results_old\",\n",
    "        \"timemoe\",\n",
    "        \"timemoe-50m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"TimesFM 200M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results_old\",\n",
    "        \"timesfm\",\n",
    "        \"timesfm-200m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    # \"Chronos 200M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results_old\",\n",
    "    #     \"chronos\",\n",
    "    #     # \"chronos_nondeterministic\",\n",
    "    #     \"chronos_base_zeroshot\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Chronos 200M Probabilistic\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     # \"chronos\",\n",
    "    #     \"chronos_nondeterministic\",\n",
    "    #     \"chronos_base_zeroshot\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    \"Dynamix\": os.path.join(WORK_DIR, \"eval_results\", \"dynamix\", data_split),\n",
    "    # \"Mean\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"mean\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Fourier\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"fourier\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metric_lists(metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def parse_value(value):\n",
    "        if isinstance(value, str):\n",
    "            stripped = value.strip()\n",
    "            if stripped.startswith(\"[\") and stripped.endswith(\"]\"):\n",
    "                inner = stripped[1:-1].strip()\n",
    "                if inner == \"\":\n",
    "                    return []\n",
    "                tokens = [token.strip() for token in inner.split(\",\")]\n",
    "                parsed_vals = []\n",
    "                for token in tokens:\n",
    "                    if token == \"\":\n",
    "                        continue\n",
    "                    lowered = token.lower()\n",
    "                    if lowered == \"nan\":\n",
    "                        parsed_vals.append(np.nan)\n",
    "                    elif lowered in {\"inf\", \"+inf\"}:\n",
    "                        parsed_vals.append(np.inf)\n",
    "                    elif lowered == \"-inf\":\n",
    "                        parsed_vals.append(-np.inf)\n",
    "                    else:\n",
    "                        parsed_vals.append(float(token))\n",
    "                return parsed_vals\n",
    "        return value\n",
    "\n",
    "    parsed = metrics_df.copy()\n",
    "    for column in parsed.columns:\n",
    "        if column == \"system\":\n",
    "            continue\n",
    "        parsed[column] = parsed[column].apply(parse_value)\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def aggregate_system_metrics(metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def aggregate_value(value):\n",
    "        if isinstance(value, list):\n",
    "            arr = np.asarray(value, dtype=float)\n",
    "            if arr.size == 0:\n",
    "                return np.nan\n",
    "            return float(np.nanmean(arr))\n",
    "        return value\n",
    "\n",
    "    aggregated = metrics_df.copy()\n",
    "    for column in aggregated.columns:\n",
    "        if column == \"system\":\n",
    "            continue\n",
    "        aggregated[column] = aggregated[column].apply(aggregate_value)\n",
    "    return aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs = defaultdict(dict)\n",
    "instance_metrics_all_runs = defaultdict(dict)\n",
    "for model_name, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "    print(model_name)\n",
    "    if not os.path.exists(run_metrics_dir):\n",
    "        print(f\"Run metrics dir does not exist: {run_metrics_dir}\")\n",
    "        continue\n",
    "    for file in sorted(\n",
    "        filter(lambda x: x.endswith(\".csv\"), os.listdir(run_metrics_dir)),\n",
    "        key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "    ):\n",
    "        if file.endswith(\".csv\"):\n",
    "            prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "            metrics_df = pd.read_csv(os.path.join(run_metrics_dir, file))\n",
    "            parsed_metrics = parse_metric_lists(metrics_df)\n",
    "            aggregated_metrics = aggregate_system_metrics(parsed_metrics)\n",
    "            metrics_all_runs[model_name][prediction_length] = aggregated_metrics.to_dict()\n",
    "            instance_metrics_all_runs[model_name][prediction_length] = parsed_metrics.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics_all_runs[\"Chronos 20M SFT\"][64][\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics = defaultdict(dict)\n",
    "for model_name, all_metrics_of_model in metrics_all_runs.items():\n",
    "    for prediction_length, metrics in all_metrics_of_model.items():\n",
    "        systems = metrics[\"system\"]\n",
    "        metrics_unrolled = {k: list(v.values()) for k, v in metrics.items() if k != \"system\"}\n",
    "        unrolled_metrics[model_name][prediction_length] = metrics_unrolled\n",
    "\n",
    "n_runs = len(unrolled_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Chronos 20M SFT\"][128][\"smape\"][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "p_values = defaultdict(dict)\n",
    "for baseline, baseline_metrics_per_pred_len in unrolled_metrics.items():\n",
    "    if baseline == \"Panda\":\n",
    "        continue\n",
    "    for prediction_length, baseline_metrics in baseline_metrics_per_pred_len.items():\n",
    "        for metric, baseline_metric_values in baseline_metrics.items():\n",
    "            if metric not in [\"mae\", \"mse\", \"smape\"]:\n",
    "                continue\n",
    "            print(f\"baseline: {baseline}, prediction_length: {prediction_length}, metric: {metric}\")\n",
    "            # number of nans in baseline_metric_values\n",
    "            print(np.isnan(baseline_metric_values).sum())\n",
    "            # mask out the nans\n",
    "            nanmask = np.isnan(baseline_metric_values)\n",
    "            baseline_metric_values_nonans = np.array(baseline_metric_values)[~nanmask]\n",
    "            panda_metric_values_nonans = np.array(unrolled_metrics[\"Panda\"][prediction_length][metric])[~nanmask]\n",
    "            result = wilcoxon(panda_metric_values_nonans, baseline_metric_values_nonans, correction=True)\n",
    "            print(result)\n",
    "            for key in [\"pvalue\", \"statistic\"]:\n",
    "                metric_key = f\"{metric}_{key}\"\n",
    "                value = getattr(result, key)\n",
    "                if metric_key in p_values[prediction_length]:\n",
    "                    p_values[prediction_length][metric_key][baseline] = value\n",
    "                else:\n",
    "                    p_values[prediction_length][metric_key] = {baseline: value}\n",
    "\n",
    "\n",
    "pvals_128 = pd.DataFrame(p_values[128]).dropna()\n",
    "pvals_256 = pd.DataFrame(p_values[256]).dropna()\n",
    "pvals_512 = pd.DataFrame(p_values[512]).dropna()\n",
    "\n",
    "for df in [pvals_128, pvals_256, pvals_512]:\n",
    "    for col in filter(lambda x: \"pvalue\" in x, df.columns):\n",
    "        if len(df[col]) > 0:  # Check if column is not empty\n",
    "            correction = multipletests(df[col])\n",
    "            df[f\"{col}_pval_adj\"] = correction[1]\n",
    "            df[f\"{col}_reject\"] = correction[0]\n",
    "\n",
    "pvals_128.to_csv(f\"{outputs_save_dir}/pvals_128.csv\")\n",
    "pvals_256.to_csv(f\"{outputs_save_dir}/pvals_256.csv\")\n",
    "pvals_512.to_csv(f\"{outputs_save_dir}/pvals_512.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = DEFAULT_COLORS[: n_runs + 1]\n",
    "default_colors = default_colors[:3] + default_colors[4:6] + default_colors[3:4]\n",
    "print(default_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"smape\"\n",
    "legend_handles = make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=selected_metric,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=default_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"{figs_save_dir}/{selected_metric}_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(25, 75),\n",
    "    whisker_percentile_range=(5, 95),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2.4, 4)},\n",
    "    box_width=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"mae\"\n",
    "legend_handles = make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=selected_metric,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=default_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"{figs_save_dir}/{selected_metric}_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(25, 75),\n",
    "    whisker_percentile_range=(5, 80),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2.4, 4)},\n",
    "    box_width=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=6,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_horizontal_patches.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=1,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_vertical_patches.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict, has_nans = get_summary_metrics_dict(unrolled_metrics, \"smape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mse\", \"mae\", \"smape\", \"spearman\"]\n",
    "metrics_dicts, has_nans = zip(*[get_summary_metrics_dict(unrolled_metrics, metric) for metric in metrics])\n",
    "all_metrics_dict = {m: metrics_dicts[i] for i, m in enumerate(metrics)}\n",
    "has_nans_dict = {m: has_nans[i] for i, m in enumerate(metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaNs for each metric and model\n",
    "nan_counts = {}\n",
    "for metric_name, metric_data in all_metrics_dict.items():\n",
    "    nan_counts[metric_name] = {}\n",
    "    for model_name, model_data in metric_data.items():\n",
    "        # all_vals = np.concatenate(model_data[\"all_vals\"])\n",
    "        all_vals_pred128 = model_data[\"all_vals\"][1]\n",
    "        nan_count = np.isnan(all_vals_pred128).sum()\n",
    "        nan_counts[metric_name][model_name] = nan_count\n",
    "        if nan_count > 0:\n",
    "            print(f\"Found {nan_count} NaNs in {model_name} for {metric_name}\")\n",
    "\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_nans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order model names by sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names_ordering = []  # sorted by median smape at 128\n",
    "# for model_name, data in all_metrics_dict[\"smape\"].items():\n",
    "#     median_metrics_128 = data[\"medians\"][1]\n",
    "#     model_names_ordering.append((model_name, median_metrics_128))\n",
    "# model_names_ordering = sorted(model_names_ordering, key=lambda x: x[1])\n",
    "# model_names_ordering = [x[0] for x in model_names_ordering]\n",
    "# print(model_names_ordering)\n",
    "\n",
    "# # Reorder all_metrics_dict according to model_names_ordering for each metric\n",
    "# reordered_metrics_dict = {}\n",
    "# for metric_name, metric_data in all_metrics_dict.items():\n",
    "#     reordered_metric_data = {}\n",
    "\n",
    "#     # Add models in the order specified by model_names_ordering\n",
    "#     for model_name in model_names_ordering:\n",
    "#         if model_name in metric_data:\n",
    "#             reordered_metric_data[model_name] = metric_data[model_name]\n",
    "#         else:\n",
    "#             raise ValueError(f\"Model {model_name} not found in {metric_name}\")\n",
    "\n",
    "#     reordered_metrics_dict[metric_name] = reordered_metric_data\n",
    "# all_metrics_dict = reordered_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pred_lengths = [128, 256, 512]\n",
    "all_pred_lengths = list(unrolled_metrics[\"Panda\"].keys())\n",
    "print(all_pred_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in unrolled_metrics.keys():\n",
    "    print(f\"========= model_name: {model_name}\")\n",
    "    for i, pred_length in enumerate(all_pred_lengths):\n",
    "        if pred_length not in selected_pred_lengths:\n",
    "            continue\n",
    "        print(f\"pred_length: {pred_length}\")\n",
    "        for metric in [\"mae\"]:\n",
    "            print(f\"{metric} median: {all_metrics_dict[metric][model_name]['medians'][i]:.2f}\")\n",
    "            print(f\"{metric} p25: {all_metrics_dict[metric][model_name]['p25'][i]:.2f}\")\n",
    "            print(f\"{metric} p75: {all_metrics_dict[metric][model_name]['p75'][i]:.2f}\")\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{x:.2f}\" for x in all_metrics_dict[\"smape\"][\"Dynamix\"][\"medians\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{x:.2f}\" for x in all_metrics_dict[\"smape\"][\"Dynamix\"][\"medians\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles = plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    metrics_to_show_envelope=[\"mae\", \"smape\"],\n",
    "    n_cols=4,\n",
    "    n_rows=1,\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_metrics_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"loc\": \"upper left\", \"frameon\": True, \"fontsize\": 10},\n",
    "    colors=default_colors,\n",
    "    use_inv_spearman=True,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder legend_handles to put Dynamix as the third key\n",
    "legend_handles_reordered = {}\n",
    "keys_order = [\"Panda\", \"Chronos 20M SFT\", \"Dynamix\", \"Time MOE 50M\", \"Chronos 200M\", \"TimesFM 200M\"]\n",
    "for key in keys_order:\n",
    "    if key in legend_handles:\n",
    "        legend_handles_reordered[key] = legend_handles[key]\n",
    "legend_handles = legend_handles_reordered\n",
    "legend_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles.values(),\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=6,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_horizontal.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles.values(),\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=1,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_vertical.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"smape\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"mae\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"mse\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    # metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=True,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 12, \"loc\": \"center right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"spearman\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [metric_to_plot],\n",
    "    metrics_to_show_envelope=[metric_to_plot],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_{metric_to_plot}_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    "    use_inv_spearman=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
