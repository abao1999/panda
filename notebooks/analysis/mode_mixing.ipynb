{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.integrate import solve_ivp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from panda.patchtst.pipeline import PatchTSTPipeline\n",
    "from panda.utils.data_utils import safe_standardize\n",
    "from panda.utils.plot_utils import apply_custom_style\n",
    "\n",
    "apply_custom_style(\"../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"pft_chattn_emb_w_poly-0\"\n",
    "pft_model = PatchTSTPipeline.from_pretrained(\n",
    "    mode=\"predict\",\n",
    "    pretrain_path=f\"/stor/work/AMDG_Gilpin_Summer2024/checkpoints/{run_name}/checkpoint-final\",\n",
    "    device_map=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_attn_maps(\n",
    "    model,\n",
    "    series: np.ndarray,\n",
    "    context_length: int,\n",
    "    linear_attn: bool = False,\n",
    "):\n",
    "    context = series[:, :context_length]\n",
    "    if context.ndim == 1:\n",
    "        context = context[None, ...]\n",
    "    context_tensor = torch.from_numpy(context).float().to(model.device)\n",
    "    pred = model(\n",
    "        context_tensor[:, -context_length:, :],\n",
    "        output_attentions=True,\n",
    "        linear_attn=linear_attn,\n",
    "    )\n",
    "    return pred.attentions\n",
    "\n",
    "\n",
    "def show_forecast(\n",
    "    model,\n",
    "    data: np.ndarray,\n",
    "    context_length: int,\n",
    "    prediction_length: int,\n",
    "    transient_length: int = 1024,\n",
    "    **kwargs,\n",
    "):\n",
    "    data = data[:, transient_length:, :]\n",
    "\n",
    "    context_data = data[:, :context_length, :]\n",
    "    stand_context = safe_standardize(context_data, axis=1)\n",
    "    predictions = (\n",
    "        model.predict(\n",
    "            torch.from_numpy(stand_context).float(),\n",
    "            prediction_length=prediction_length,\n",
    "            **kwargs,\n",
    "        )[:, 0, ...]\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    stand_gt = safe_standardize(\n",
    "        data[:, context_length : context_length + prediction_length, :],\n",
    "        context=data[:, :context_length, :],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    context_ts = np.arange(0, context_length)\n",
    "    prediction_ts = np.arange(context_length, context_length + prediction_length)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(data.shape[-1]):\n",
    "        plt.plot(context_ts, stand_context[0, :, i], color=\"k\", alpha=0.5)\n",
    "        plt.plot(prediction_ts, stand_gt[0, :, i], color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.plot(prediction_ts, predictions[0, :, i], color=\"r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def interaction_index(matrix: np.ndarray, axes: tuple[int, int] = (-2, -1)) -> np.ndarray:\n",
    "    fronorm = np.linalg.norm(matrix, axis=axes, ord=\"fro\")\n",
    "    twonorm = np.linalg.norm(matrix, axis=axes, ord=2)\n",
    "    return 1 - fronorm / (twonorm * np.sqrt(matrix.shape[axes[0]]))\n",
    "\n",
    "\n",
    "def mean_row_entropy(matrix: np.ndarray, axis: int = -1, eps: float = 1e-10) -> np.ndarray:\n",
    "    assert np.allclose(matrix.sum(axis=axis), 1), \"All rows must be a probability distribution\"\n",
    "    return -np.sum(matrix * np.log(matrix + eps), axis=axis).mean(axis=axis)\n",
    "\n",
    "\n",
    "def fronorm(matrix: np.ndarray, axes: tuple[int, int] = (-2, -1)) -> np.ndarray:\n",
    "    return np.linalg.norm(matrix, axis=axes, ord=\"fro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout(attention_stack, skip_connection=True, start_layer=-1, stop_layer=None):\n",
    "    \"\"\"\n",
    "    Computes the attention rollout for a stack of attention matrices.\n",
    "    Based on the description in Abnar & Zuidema\n",
    "    https://arxiv.org/pdf/2005.00928\n",
    "\n",
    "    Args:\n",
    "        attention_stack (torch.Tensor): Tensor of shape (L, *, C, C) containing L attention\n",
    "            matrices.\n",
    "        skip_connection (bool): If True, adds an identity matrix to each attention matrix\n",
    "            to account for residual connections.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A (*, C, C) rollout attention matrix.\n",
    "    \"\"\"\n",
    "    L, *_, C, _ = attention_stack.shape\n",
    "    rollouts = torch.zeros(L + 1, *attention_stack.shape[1:], device=attention_stack.device)\n",
    "    rollout = torch.eye(C, device=attention_stack.device)[None, ...]\n",
    "    rollouts[0] = rollout\n",
    "    for i in range(L):\n",
    "        A = attention_stack[i]\n",
    "        if skip_connection:\n",
    "            A += torch.eye(C, device=A.device)\n",
    "            A /= A.sum(dim=-1, keepdim=True)\n",
    "        rollout = rollout @ A\n",
    "        rollouts[i + 1] = rollout\n",
    "    rollout_result = rollouts[start_layer:stop_layer]\n",
    "    torch.cuda.empty_cache()\n",
    "    del rollouts\n",
    "    return rollout_result\n",
    "\n",
    "\n",
    "def single_head_attn_rollout(\n",
    "    model,\n",
    "    data: np.ndarray,\n",
    "    context_length: int = 512,\n",
    "    attention_type: str = \"temporal\",\n",
    "    **kwargs,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the attention rollout for the whole model by averaging over heads.\n",
    "    \"\"\"\n",
    "    bs, _, num_channels = data.shape\n",
    "    attn_weights = extract_attn_maps(\n",
    "        model.model,\n",
    "        data,\n",
    "        context_length,\n",
    "        linear_attn=False,\n",
    "    )\n",
    "    if attention_type == \"channel\":\n",
    "        attn_weights = attn_weights[1::2]\n",
    "    elif attention_type == \"temporal\":\n",
    "        attn_weights = attn_weights[0::2]\n",
    "    else:\n",
    "        raise ValueError(\"Attention type must be either 'channel' or 'temporal'\")\n",
    "\n",
    "    # average over heads\n",
    "    # if attention_type is channel\n",
    "    # shape: (num_layers, batch_size*num_tokens, num_channels, num_channels)\n",
    "    # if attention_type is temporal\n",
    "    # shape: (num_layers, batch_size*num_channels, num_tokens, num_tokens)\n",
    "    attn_weights = torch.stack(attn_weights, dim=0).mean(dim=2)\n",
    "\n",
    "    num_tokens = context_length // model.model.config.patch_length\n",
    "    n = num_tokens if attention_type == \"temporal\" else num_channels\n",
    "    m = num_channels if attention_type == \"temporal\" else num_tokens\n",
    "    rollouts = (attention_rollout(attn_weights, **kwargs).detach().cpu().numpy()).reshape(-1, bs, m, n, n)\n",
    "    del attn_weights\n",
    "    torch.cuda.empty_cache()\n",
    "    return rollouts\n",
    "\n",
    "\n",
    "def multi_head_attn_rollout(\n",
    "    model, data: np.ndarray, context_length: int = 512, attention_type: str = \"temporal\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute attention rollout for each head by averaging upstream heads\"\"\"\n",
    "    bs, _, num_channels = data.shape\n",
    "    attn_weights = extract_attn_maps(\n",
    "        model.model,\n",
    "        data,\n",
    "        context_length,\n",
    "        linear_attn=False,\n",
    "    )\n",
    "    if attention_type == \"channel\":\n",
    "        attn_weights = attn_weights[1::2]\n",
    "    elif attention_type == \"temporal\":\n",
    "        attn_weights = attn_weights[0::2]\n",
    "    else:\n",
    "        raise ValueError(\"Attention type must be either 'channel' or 'temporal'\")\n",
    "\n",
    "    # shape: (num_layers, batch_size*num_channels, num_heads, num_tokens, num_tokens)\n",
    "    attn_weights = torch.stack(attn_weights, dim=0)\n",
    "    L, S, H, C, _ = attn_weights.shape\n",
    "\n",
    "    # shape: (num_layers, bs, num_tokens, num_tokens)\n",
    "    single_head_rollouts = attention_rollout(attn_weights.mean(dim=2), start_layer=0, stop_layer=L)\n",
    "    # shape: (num_layers, bs, num_heads, num_tokens, num_tokens)\n",
    "    rollouts = attn_weights @ single_head_rollouts.unsqueeze(2)\n",
    "\n",
    "    num_tokens = context_length // model.model.config.patch_length\n",
    "    n = num_tokens if attention_type == \"temporal\" else num_channels\n",
    "    m = num_channels if attention_type == \"temporal\" else num_tokens\n",
    "\n",
    "    # shape: (num_layers, batch_size, channels or tokens, num_heads, tokens or channels, tokens or channels)\n",
    "    return rollouts.reshape(L, bs, m, H, n, n).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CoupledOscillator:\n",
    "    num_oscillators: int\n",
    "    w0: float = 1.0\n",
    "\n",
    "    initial_conditions: np.ndarray | None = None\n",
    "    spring_constants: np.ndarray | None = None\n",
    "    masses: np.ndarray | None = None\n",
    "    stencil: np.ndarray | None = None\n",
    "\n",
    "    tspan: tuple[float, float] = (0, 100)\n",
    "    num_eval_points: int = 1024\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.spring_constants is None:\n",
    "            self.spring_constants = np.ones(self.num_oscillators)\n",
    "\n",
    "        if self.masses is None:\n",
    "            self.masses = np.ones(self.num_oscillators)\n",
    "\n",
    "        if self.initial_conditions is None:\n",
    "            self.initial_conditions = np.random.randn(2 * self.dim)\n",
    "            self.initial_conditions[self.dim :] = 0\n",
    "\n",
    "        if self.stencil is None:\n",
    "            self.stencil = np.zeros((self.num_oscillators, self.num_oscillators))\n",
    "            for i in range(self.num_oscillators):\n",
    "                self.stencil[i, (i - 1) % self.num_oscillators] = self.spring_constants[(i - 1) % self.num_oscillators]\n",
    "                self.stencil[i, (i - 1) % self.num_oscillators] /= self.masses[(i - 1) % self.num_oscillators]\n",
    "                self.stencil[i, (i + 1) % self.num_oscillators] = self.spring_constants[(i + 1) % self.num_oscillators]\n",
    "                self.stencil[i, (i + 1) % self.num_oscillators] /= self.masses[(i + 1) % self.num_oscillators]\n",
    "                self.stencil[i, i] = -2 * self.spring_constants[i] / self.masses[i]\n",
    "\n",
    "        self.ts = np.linspace(self.tspan[0], self.tspan[1], self.num_eval_points)\n",
    "\n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self.num_oscillators\n",
    "\n",
    "    @property\n",
    "    def basis(self) -> np.ndarray:\n",
    "        return np.linalg.eigh(self.stencil)\n",
    "\n",
    "    def __call__(self, t: float, uv: np.ndarray) -> np.ndarray:\n",
    "        u, v = uv[: self.dim], uv[self.dim :]\n",
    "        dudt = v\n",
    "        dvdt = self.stencil @ u\n",
    "        return np.concatenate([dudt, dvdt])\n",
    "\n",
    "    def integrate(self) -> np.ndarray:\n",
    "        sol = solve_ivp(self, self.tspan, self.initial_conditions, t_eval=self.ts)\n",
    "        return sol.y[: self.dim].T\n",
    "\n",
    "    def __getitem__(self, idx: int | slice) -> np.ndarray:\n",
    "        if isinstance(idx, int):\n",
    "            return self.integrate()\n",
    "        elif isinstance(idx, slice):\n",
    "            inds = np.arange(idx.start, idx.stop, idx.step)\n",
    "            solutions = np.zeros((len(inds), self.num_eval_points, self.dim))\n",
    "            for i, ind in tqdm(enumerate(inds), total=len(inds)):\n",
    "                solutions[i] = self.integrate()\n",
    "            return solutions\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid index: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oscillators = 8\n",
    "masses = np.ones(num_oscillators) * 0.01\n",
    "springs = np.ones(num_oscillators) * 0.1\n",
    "\n",
    "series_fn = CoupledOscillator(\n",
    "    num_oscillators=num_oscillators,\n",
    "    masses=masses,\n",
    "    spring_constants=springs,\n",
    "    num_eval_points=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_forecast(\n",
    "    pft_model,\n",
    "    series_fn[0:1],\n",
    "    context_length=512,\n",
    "    prediction_length=512,\n",
    "    limit_prediction_length=False,\n",
    "    sliding_context=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlehead_rollouts = single_head_attn_rollout(\n",
    "    pft_model.model,\n",
    "    series_fn[0:1],\n",
    "    context_length=1024,\n",
    "    attention_type=\"channel\",\n",
    "    start_layer=0,\n",
    "    stop_layer=None,\n",
    ")[:, 0, ...]\n",
    "singlehead_rollouts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idx = -1\n",
    "fig, axes = plt.subplots(1, singlehead_rollouts.shape[0], figsize=(singlehead_rollouts.shape[0] * 2, 2))\n",
    "plt.subplots_adjust(wspace=0.0)\n",
    "for i in range(singlehead_rollouts.shape[0]):\n",
    "    axes[i].imshow(singlehead_rollouts[i, token_idx], cmap=\"magma\")\n",
    "    axes[i].set_axis_off()\n",
    "fig.supxlabel(\"Layer\", fontsize=20, y=-0.05, x=0.50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_maps = extract_attn_maps(\n",
    "    pft_model.model,\n",
    "    series_fn[0:1],\n",
    "    context_length=1024,\n",
    "    linear_attn=False,\n",
    ")\n",
    "len(attn_maps), attn_maps[0].shape, attn_maps[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = -1\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.0, hspace=0.05)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        amap = attn_maps[2 * j + 1][sample_idx, i].detach().cpu().numpy()\n",
    "        axes[i, j].imshow(amap, cmap=\"magma\")\n",
    "        axes[i, j].set_axis_off()\n",
    "fig.supxlabel(\"Layer\", fontsize=20, y=0.08, x=0.51)\n",
    "fig.supylabel(\"Head\", fontsize=20, x=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_rollouts = multi_head_attn_rollout(\n",
    "    pft_model.model, series_fn[0:1], context_length=1024, attention_type=\"channel\"\n",
    ")[:, 0, ...]\n",
    "multihead_rollouts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idx = -1\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.0, hspace=0.05)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        amap = multihead_rollouts[j, token_idx, i]\n",
    "        axes[i, j].imshow(amap, cmap=\"magma\")\n",
    "        axes[i, j].set_axis_off()\n",
    "\n",
    "fig.supxlabel(\"Layer\", fontsize=20, y=0.08, x=0.51)\n",
    "fig.supylabel(\"Head\", fontsize=20, x=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine(\n",
    "    ts: np.ndarray,\n",
    "    freqs: float | np.ndarray,\n",
    "    phi: float | np.ndarray = 0.0,\n",
    "    amp: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    freqs = freqs if isinstance(freqs, float) else freqs[..., None]\n",
    "    phi = phi if isinstance(phi, float) else phi[..., None]\n",
    "    return amp * np.sin(freqs * ts + phi).transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 1 / 4\n",
    "max_freq = 2\n",
    "resolution = 8\n",
    "freqs = (\n",
    "    2\n",
    "    * np.pi\n",
    "    * np.stack(\n",
    "        np.mgrid[min_freq : max_freq : resolution * 1j, min_freq : max_freq : resolution * 1j],\n",
    "        axis=-1,\n",
    "    ).reshape(resolution * resolution, 2)\n",
    ")\n",
    "\n",
    "ts = np.linspace(0, 100, 4096)\n",
    "sines = sine(ts, freqs, phi=np.array([0.0, 1.0]))\n",
    "freqs.shape, sines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_forecast(\n",
    "    pft_model,\n",
    "    sines[-2:-1],\n",
    "    context_length=512,\n",
    "    prediction_length=512,\n",
    "    limit_prediction_length=False,\n",
    "    sliding_context=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = single_head_attn_rollout(\n",
    "    pft_model.model,\n",
    "    sines,\n",
    "    context_length=512,\n",
    "    attention_type=\"temporal\",\n",
    "    start_layer=-2,\n",
    ")[0].mean(axis=1)  # average over channels/patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.0, hspace=0.05)\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i % resolution > i // resolution:\n",
    "        ax.set_axis_off()\n",
    "        continue\n",
    "    ax.imshow(rollouts[i], cmap=\"magma\")\n",
    "    ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_sines_freq_and_phase(\n",
    "    model,\n",
    "    freqs: np.ndarray,\n",
    "    phases: np.ndarray,\n",
    "    batch_size: int = 1024,\n",
    "    context_length: int = 1024,\n",
    "    attention_type: str = \"temporal\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sweep through a grid of frequencies and phases, and compute the rollout of the model for each pair.\n",
    "    \"\"\"\n",
    "    freq_resolution = freqs.shape[0]\n",
    "    phase_resolution = phases.shape[0]\n",
    "    response = np.zeros(freq_resolution * phase_resolution)\n",
    "    sweepinds = np.indices([freq_resolution, phase_resolution]).reshape(2, -1)\n",
    "    dummy = np.ones(min(batch_size, freq_resolution * phase_resolution))\n",
    "\n",
    "    for i in tqdm(\n",
    "        range(0, freq_resolution * phase_resolution, batch_size),\n",
    "        desc=\"Processing batch sweeps\",\n",
    "    ):\n",
    "        freq_batch = np.stack([freqs[sweepinds[0, i : i + batch_size]], dummy], axis=-1)\n",
    "        phases_batch = np.stack([phases[sweepinds[1, i : i + batch_size]], dummy], axis=-1)\n",
    "        sines_batch = sine(ts, freq_batch, phi=phases_batch)\n",
    "\n",
    "        # shape: (batch_size, num_channels, context_length//patch_length, context_length//patch_length)\n",
    "        rollouts = single_head_attn_rollout(\n",
    "            model,\n",
    "            sines_batch,\n",
    "            context_length=context_length,\n",
    "            attention_type=attention_type,\n",
    "            start_layer=-1,\n",
    "        )[0].mean(axis=1)\n",
    "        response[i : i + batch_size] = np.linalg.norm(rollouts, axis=(1, 2), ord=2)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        del rollouts\n",
    "\n",
    "    return response.reshape(freq_resolution, phase_resolution)\n",
    "\n",
    "\n",
    "def sweep_sines_freqs(\n",
    "    model,\n",
    "    freqs1: np.ndarray,\n",
    "    freqs2: np.ndarray,\n",
    "    batch_size: int = 1024,\n",
    "    context_length: int = 1024,\n",
    "    attention_type: str = \"temporal\",\n",
    "    num_waves: int = 2,\n",
    "    amp: float = 1.0,\n",
    "    phase: float | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sweep through a grid of frequencies and phases, and compute the rollout of the model for each pair.\n",
    "    \"\"\"\n",
    "    freq_resolution1 = freqs1.shape[0]\n",
    "    freq_resolution2 = freqs2.shape[0]\n",
    "    response = np.zeros(freq_resolution1 * freq_resolution2)\n",
    "    sweepinds = np.indices([freq_resolution1, freq_resolution2]).reshape(2, -1)\n",
    "    dummy_freqs = np.random.rand(min(batch_size, freq_resolution1 * freq_resolution2), num_waves - 2)\n",
    "    dummy_phases = np.random.rand(num_waves)\n",
    "    dummy_phases[0] = 0\n",
    "    if phase is not None:\n",
    "        dummy_phases[1] = phase\n",
    "\n",
    "    for i in tqdm(\n",
    "        range(0, freq_resolution1 * freq_resolution2, batch_size),\n",
    "        desc=\"Processing batch sweeps\",\n",
    "    ):\n",
    "        freq_batch = np.stack(\n",
    "            [\n",
    "                freqs1[sweepinds[0, i : i + batch_size]],\n",
    "                freqs2[sweepinds[1, i : i + batch_size]],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        freq_batch = np.c_[freq_batch, dummy_freqs]\n",
    "        sines_batch = sine(\n",
    "            ts,\n",
    "            freq_batch,\n",
    "            phi=dummy_phases.reshape(1, -1),\n",
    "            amp=amp,\n",
    "        )\n",
    "\n",
    "        # shape: (batch_size, num_channels, context_length//patch_length, context_length//patch_length)\n",
    "        rollouts = single_head_attn_rollout(\n",
    "            model,\n",
    "            sines_batch,\n",
    "            context_length=context_length,\n",
    "            attention_type=attention_type,\n",
    "            start_layer=-1,\n",
    "        )[0]\n",
    "        # response[i : i + batch_size] = interaction_index(rollouts).mean(axis=1)\n",
    "        response[i : i + batch_size] = mean_row_entropy(rollouts).mean(axis=1)\n",
    "        torch.cuda.empty_cache()\n",
    "        del rollouts\n",
    "\n",
    "    return response.reshape(freq_resolution1, freq_resolution2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 256\n",
    "bounds = (0.5, 2.5)\n",
    "freqs1 = 2 * np.pi * np.linspace(bounds[0], bounds[1], resolution)\n",
    "freqs2 = 2 * np.pi * np.linspace(bounds[0], bounds[1], resolution)\n",
    "\n",
    "response = sweep_sines_freqs(\n",
    "    pft_model.model,\n",
    "    freqs1,\n",
    "    freqs2,\n",
    "    batch_size=4096,\n",
    "    context_length=512,\n",
    "    attention_type=\"temporal\",\n",
    "    num_waves=2,\n",
    "    amp=1,\n",
    "    phase=np.pi,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(\n",
    "    response,\n",
    "    cmap=\"inferno\",\n",
    "    origin=\"lower\",\n",
    "    extent=(0, freqs1.shape[0], 0, freqs2.shape[0]),\n",
    ")\n",
    "\n",
    "tlocs1 = np.arange(0, freqs1.shape[0] + freqs1.shape[0] // 4, freqs1.shape[0] // 4)\n",
    "tlocs1[-1] -= 1\n",
    "\n",
    "tlocs2 = np.arange(0, freqs2.shape[0] + freqs2.shape[0] // 4, freqs2.shape[0] // 4)\n",
    "tlocs2[-1] -= 1\n",
    "\n",
    "lower = bounds[0] * 2\n",
    "upper = bounds[1] * 2\n",
    "locs = np.linspace(lower, upper, 5)\n",
    "locs = [l if not l.is_integer() else int(l) for l in locs]\n",
    "locs_str = [r\"$\\mathbf{\" + (str(s) if s != 1 else \"\") + r\"\\pi}$\" if s != \"0\" else \"0\" for s in locs]\n",
    "\n",
    "plt.yticks(tlocs1, locs_str, fontweight=\"bold\", fontsize=16)\n",
    "plt.xticks(tlocs2, locs_str, fontweight=\"bold\", fontsize=16)\n",
    "cbar = plt.colorbar(shrink=0.8175, pad=0.0)\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig(\"../figures/nonlin_resonance.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_bispectra_scaling(\n",
    "    checkpoint_dir: str,\n",
    "    freqs1: np.ndarray,\n",
    "    freqs2: np.ndarray,\n",
    "    context_length: int = 512,\n",
    "    batch_size: int = 2048,\n",
    "    attention_type: str = \"temporal\",\n",
    "    num_trials: int = 5,\n",
    "    default_seed: int = 0,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Plots the bispectra across all checkpoints in the given directory.\n",
    "\n",
    "    Assumes that the checkpoint directory contains checkpoint-{i} folders for i in [0, 1, 2, ...]\n",
    "    and a single checkpoint-final folder.\n",
    "    \"\"\"\n",
    "    checkpoints = os.listdir(checkpoint_dir)\n",
    "    checkpoints.remove(\"checkpoint-final\")\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    iterations = np.array([int(checkpoint.split(\"-\")[-1]) for checkpoint in checkpoints])\n",
    "    rngs = np.random.default_rng(default_seed).spawn(len(checkpoints))\n",
    "\n",
    "    mean_interactions = np.zeros(len(checkpoints))\n",
    "    std_interactions = np.zeros(len(checkpoints))\n",
    "    for i, checkpoint in enumerate(tqdm(checkpoints, desc=\"Processing checkpoints\")):\n",
    "        model = PatchTSTPipeline.from_pretrained(\n",
    "            mode=\"predict\",\n",
    "            pretrain_path=f\"{checkpoint_dir}/{checkpoint}\",\n",
    "            device_map=\"cuda:0\",\n",
    "        )\n",
    "        # shape: (resolution, resolution)\n",
    "        trial_interactions = np.zeros(num_trials)\n",
    "        for j, randphase in enumerate(rngs[i].uniform(0, 2 * np.pi, num_trials)):\n",
    "            response = sweep_sines_freqs(\n",
    "                model,\n",
    "                freqs1,\n",
    "                freqs2,\n",
    "                batch_size=batch_size,\n",
    "                context_length=context_length,\n",
    "                attention_type=attention_type,\n",
    "                phase=randphase,\n",
    "                num_waves=2,\n",
    "            )\n",
    "            # Compute normalized off-diagonal activity\n",
    "            diag_mask = np.eye(response.shape[0], dtype=bool)\n",
    "            off_diag_norm = np.linalg.norm(response[~diag_mask])\n",
    "            diag_norm = np.linalg.norm(response[diag_mask])\n",
    "            trial_interactions[j] = off_diag_norm / diag_norm\n",
    "        mean_interactions[i] = trial_interactions.mean()\n",
    "        std_interactions[i] = trial_interactions.std()\n",
    "\n",
    "        # cleanup manually\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return iterations, mean_interactions, std_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 128\n",
    "freqs1 = 2 * np.pi * np.linspace(bounds[0], bounds[1], resolution)\n",
    "freqs2 = 2 * np.pi * np.linspace(bounds[0], bounds[1], resolution)\n",
    "\n",
    "work_dir = os.environ[\"WORK\"]\n",
    "iterations, mean_interactions, std_interactions = sine_bispectra_scaling(\n",
    "    f\"{work_dir}/checkpoints/pft_rff496_proj-0\",\n",
    "    freqs1,\n",
    "    freqs2,\n",
    "    context_length=512,\n",
    "    batch_size=4096,\n",
    "    attention_type=\"temporal\",\n",
    "    num_trials=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../outputs/modemix\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "np.save(f\"{output_dir}/sine_mean_interactions.npy\", mean_interactions)\n",
    "np.save(f\"{output_dir}/sine_std_interactions.npy\", std_interactions)\n",
    "np.save(f\"{output_dir}/sine_iterations.npy\", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_interactions, std_interactions, iterations = (\n",
    "    np.load(f\"{output_dir}/sine_mean_interactions.npy\"),\n",
    "    np.load(f\"{output_dir}/sine_std_interactions.npy\"),\n",
    "    np.load(f\"{output_dir}/sine_iterations.npy\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cutoff = 0\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(\n",
    "    iterations[checkpoint_cutoff:],\n",
    "    mean_interactions[checkpoint_cutoff:],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-.\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    iterations[checkpoint_cutoff:],\n",
    "    mean_interactions[checkpoint_cutoff:] - std_interactions[checkpoint_cutoff:],\n",
    "    mean_interactions[checkpoint_cutoff:] + std_interactions[checkpoint_cutoff:],\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0, 0))\n",
    "plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "plt.xticks(iterations[checkpoint_cutoff:])\n",
    "plt.xlabel(\"Checkpoint Iteration\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Response Interaction Index\", fontweight=\"bold\")\n",
    "plt.savefig(\"../figures/sine_bispectra_scaling.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "from panda.utils import get_system_filepaths, load_trajectory_from_arrow\n",
    "\n",
    "\n",
    "def data_bispectra_scaling(\n",
    "    checkpoint_dir: str,\n",
    "    data_dir: str,\n",
    "    context_length: int = 512,\n",
    "    num_windows: int = 10,\n",
    "    attention_type: str = \"temporal\",\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes the bispectra across all checkpoints in the given directory.\n",
    "\n",
    "    Assumes that the checkpoint directory contains checkpoint-{i} folders for i in [0, 1, 2, ...]\n",
    "    and a single checkpoint-final folder.\n",
    "    \"\"\"\n",
    "    checkpoints = os.listdir(checkpoint_dir)\n",
    "    checkpoints.remove(\"checkpoint-final\")\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    iterations = np.array([int(checkpoint.split(\"-\")[-1]) for checkpoint in checkpoints])\n",
    "    system_dirs = glob(f\"{data_dir}/*\")\n",
    "\n",
    "    mean_interactions = np.zeros(len(checkpoints))\n",
    "    std_interactions = np.zeros(len(checkpoints))\n",
    "    for i, checkpoint in enumerate(tqdm(checkpoints, desc=\"Processing checkpoints\")):\n",
    "        model = PatchTSTPipeline.from_pretrained(\n",
    "            mode=\"predict\",\n",
    "            pretrain_path=f\"{checkpoint_dir}/{checkpoint}\",\n",
    "            device_map=\"cuda:0\",\n",
    "        )\n",
    "        trial_interactions = np.zeros(len(system_dirs))\n",
    "        for j, system_dir in enumerate(system_dirs):\n",
    "            # pick a single random system to evaluate\n",
    "            syspaths = get_system_filepaths(system_dir, data_dir, \"test_zeroshot\")\n",
    "            syspath = np.random.choice(syspaths)\n",
    "            traj, _ = load_trajectory_from_arrow(syspath)\n",
    "\n",
    "            # split into random context windows\n",
    "            window_idxs = np.random.randint(0, traj.shape[1] - context_length, num_windows)\n",
    "            windows = np.stack([traj[:, w : w + context_length].T for w in window_idxs])\n",
    "\n",
    "            attn_weights = extract_attn_maps(\n",
    "                model.model,\n",
    "                windows,\n",
    "                context_length,\n",
    "                linear_attn=False,\n",
    "            )\n",
    "            attnmap = attn_weights[-2 if attention_type == \"temporal\" else -1].cpu().detach().numpy()\n",
    "            interactions = mean_row_entropy(attnmap)\n",
    "            # rollouts = single_head_attn_rollout(\n",
    "            #     model, windows, context_length, attention_type\n",
    "            # )[0]\n",
    "            # interactions = mean_row_entropy(rollouts)\n",
    "            trial_interactions[j] = interactions.mean()\n",
    "        mean_interactions[i] = trial_interactions.mean()\n",
    "        std_interactions[i] = trial_interactions.std() / np.sqrt(trial_interactions.shape[0])\n",
    "        # cleanup manually\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return iterations, mean_interactions, std_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = os.environ[\"WORK\"]\n",
    "iterations, mean_interactions, std_interactions = data_bispectra_scaling(\n",
    "    f\"{work_dir}/checkpoints/pft_rff496_proj-0\",\n",
    "    f\"{work_dir}/data/improved/final_skew40/test_zeroshot\",\n",
    "    context_length=512,\n",
    "    attention_type=\"temporal\",\n",
    "    num_windows=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../outputs/modemix\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "np.save(f\"{output_dir}/data_mean_interactions.npy\", mean_interactions)\n",
    "np.save(f\"{output_dir}/data_std_interactions.npy\", std_interactions)\n",
    "np.save(f\"{output_dir}/data_iterations.npy\", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_interactions, std_interactions, iterations = (\n",
    "    np.load(f\"{output_dir}/data_mean_interactions.npy\"),\n",
    "    np.load(f\"{output_dir}/data_std_interactions.npy\"),\n",
    "    np.load(f\"{output_dir}/data_iterations.npy\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cutoff = 0\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(\n",
    "    iterations[checkpoint_cutoff:],\n",
    "    mean_interactions[checkpoint_cutoff:],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    markersize=10,\n",
    ")\n",
    "plt.fill_between(\n",
    "    iterations[checkpoint_cutoff:],\n",
    "    mean_interactions[checkpoint_cutoff:] - std_interactions[checkpoint_cutoff:],\n",
    "    mean_interactions[checkpoint_cutoff:] + std_interactions[checkpoint_cutoff:],\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0, 0))\n",
    "plt.gca().xaxis.get_offset_text().set_fontsize(16)\n",
    "plt.xticks(iterations[checkpoint_cutoff:], fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel(\"Checkpoint Iteration\", fontweight=\"bold\", fontsize=16)\n",
    "plt.ylabel(\"Final layer row-entropy\", fontweight=\"bold\", fontsize=16)\n",
    "plt.savefig(\"../figures/data_bispectra_scaling.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
