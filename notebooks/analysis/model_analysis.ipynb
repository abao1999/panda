{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from panda.patchtst.pipeline import PatchTSTPipeline\n",
    "from panda.utils.data_utils import (\n",
    "    get_system_filepaths,\n",
    "    load_trajectory_from_arrow,\n",
    ")\n",
    "from panda.utils.plot_utils import apply_custom_style\n",
    "\n",
    "apply_custom_style(\"../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = \"../figures/attnmaps\"\n",
    "os.makedirs(fig_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"pft_chattn_emb_w_poly-0\"\n",
    "pft_model = PatchTSTPipeline.from_pretrained(\n",
    "    mode=\"predict\",\n",
    "    pretrain_path=f\"/stor/work/AMDG_Gilpin_Summer2024/checkpoints/{run_name}/checkpoint-final\",\n",
    "    device_map=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Kernel\" in str(pft_model.model.model.encoder.embedder):\n",
    "    emb_proj = pft_model.model.model.encoder.embedder.projection.weight.detach().cpu().numpy()\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(np.log(np.abs(emb_proj) ** 2), cmap=\"magma\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_weights(model, key: str) -> list[dict[str, torch.Tensor]]:\n",
    "    params = [\n",
    "        {\n",
    "            \"Wq\": getattr(l, key).q_proj.weight,\n",
    "            \"Wk\": getattr(l, key).k_proj.weight,\n",
    "            \"Wv\": getattr(l, key).v_proj.weight,\n",
    "        }\n",
    "        for l in model.model.model.encoder.layers  # lol\n",
    "    ]\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_attn_map(weights: list[dict[str, torch.Tensor]], index: int, shift: bool = False) -> np.ndarray:\n",
    "    attn_map = (weights[index][\"Wq\"] @ weights[index][\"Wk\"].T).detach().cpu().numpy()\n",
    "    if shift:\n",
    "        attn_map = (attn_map - np.min(attn_map)) / (np.max(attn_map) - np.min(attn_map))\n",
    "    return attn_map\n",
    "\n",
    "\n",
    "def symmetric_distance(attn_map: np.ndarray) -> float:\n",
    "    return 0.5 * np.linalg.norm(attn_map - attn_map.T, \"fro\") / np.linalg.norm(attn_map, \"fro\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_weights = get_attn_weights(pft_model, \"temporal_self_attn\")\n",
    "channel_weights = get_attn_weights(pft_model, \"channel_self_attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_map = get_attn_map(temporal_weights, 0)\n",
    "print(symmetric_distance(attn_map))\n",
    "plt.figure()\n",
    "plt.imshow(np.log(attn_map**2), cmap=\"RdBu\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_map = get_attn_map(channel_weights, 0)\n",
    "print(symmetric_distance(attn_map))\n",
    "plt.figure()\n",
    "plt.imshow(np.log(attn_map**2), cmap=\"RdBu\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    attn_map = get_attn_map(temporal_weights, i)\n",
    "    ax.imshow(attn_map, cmap=\"magma\")\n",
    "    ax.set_title(f\"Layer {i}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attn_map(\n",
    "    model,\n",
    "    context: np.ndarray,\n",
    "    sample_idx: int,\n",
    "    layer_idx: int,\n",
    "    head_idx: int,\n",
    "    prefix: str = \"\",\n",
    "    colormap: str = \"magma\",\n",
    "    show_colorbar: bool = True,\n",
    "    show_title: bool = True,\n",
    "    save_path: str | None = None,\n",
    "    linear_attn: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Plot attention matrix with corresponding timeseries patches along edges.\"\"\"\n",
    "    attention_type = \"temporal\" if layer_idx % 2 == 0 else \"channel\"\n",
    "    patch_size = model.config.patch_length\n",
    "    patches = context.reshape(context.shape[0], -1, patch_size)\n",
    "    if attention_type == \"channel\":\n",
    "        patches = patches.transpose(1, 0, 2)\n",
    "\n",
    "    context_tensor = torch.from_numpy(context.T).float().to(pft_model.device)[None, ...]\n",
    "    pred = model(context_tensor, linear_attn=linear_attn, output_attentions=True)\n",
    "    attn_weights = pred.attentions\n",
    "\n",
    "    # Extract attention weights for specified sample, layer and head\n",
    "    num_samples = attn_weights[layer_idx].shape[0]\n",
    "    attn = attn_weights[layer_idx][sample_idx, head_idx].detach().cpu().numpy()\n",
    "    n_patches = attn.shape[0]\n",
    "\n",
    "    # Create figure with gridspec layout\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Create main grid with padding for colorbar\n",
    "    outer_grid = fig.add_gridspec(1, 2, width_ratios=[1, 0.05], wspace=0.05)\n",
    "\n",
    "    # Create sub-grid for the plots\n",
    "    gs = outer_grid[0].subgridspec(2, 2, width_ratios=[0.15, 0.85], height_ratios=[0.15, 0.85], wspace=0, hspace=0)\n",
    "\n",
    "    # Plot attention matrix first to get its size\n",
    "    ax_main = fig.add_subplot(gs[1, 1])\n",
    "    im = ax_main.imshow(attn, extent=(0, n_patches, n_patches, 0), cmap=colormap)\n",
    "    ax_main.set_xticks([])\n",
    "    ax_main.set_yticks([])\n",
    "\n",
    "    linewidth = 2\n",
    "    # Plot patches along top\n",
    "    ax_top = fig.add_subplot(gs[0, 1])\n",
    "    for i in range(n_patches):\n",
    "        x = np.linspace(i, i + 1, patch_size)\n",
    "        ax_top.plot(x, patches[sample_idx, i], linewidth=linewidth)\n",
    "    ax_top.set_xlim(0, n_patches)\n",
    "    ax_top.set_xticks([])\n",
    "    ax_top.set_yticks([])\n",
    "    ax_top.grid(True)\n",
    "\n",
    "    # Plot patches along left side\n",
    "    ax_left = fig.add_subplot(gs[1, 0])\n",
    "    for i in range(n_patches):\n",
    "        y = np.linspace(i, i + 1, patch_size)\n",
    "        ax_left.plot(-patches[sample_idx, i], y, linewidth=linewidth)\n",
    "    ax_left.set_ylim(n_patches, 0)\n",
    "    ax_left.set_xticks([])\n",
    "    ax_left.set_yticks([])\n",
    "    ax_left.grid(True)\n",
    "\n",
    "    ax_cbar = fig.add_subplot(outer_grid[1])\n",
    "    if show_colorbar:\n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, cax=ax_cbar)\n",
    "\n",
    "    else:\n",
    "        # remove outer_grid[1]\n",
    "        fig.delaxes(ax_cbar)\n",
    "    # Remove empty subplot\n",
    "    fig.delaxes(fig.add_subplot(gs[0, 0]))\n",
    "\n",
    "    # Force exact alignment of subplots\n",
    "    main_pos = ax_main.get_position()\n",
    "    ax_top.set_position(\n",
    "        [main_pos.x0, main_pos.y1, main_pos.width, ax_top.get_position().height]  # type: ignore\n",
    "    )\n",
    "    ax_left.set_position(\n",
    "        [\n",
    "            ax_left.get_position().x0,\n",
    "            main_pos.y0,\n",
    "            ax_left.get_position().width,\n",
    "            main_pos.height,\n",
    "        ]  # type: ignore\n",
    "    )\n",
    "    ax_cbar.set_position(\n",
    "        [\n",
    "            ax_cbar.get_position().x0,\n",
    "            main_pos.y0,\n",
    "            ax_cbar.get_position().width,\n",
    "            main_pos.height,\n",
    "        ]  # type: ignore\n",
    "    )\n",
    "    sample_type = \"channel\" if attention_type == \"temporal\" else \"patch\"\n",
    "    if show_title:\n",
    "        ax_top.set_title(\n",
    "            f\"{prefix} {attention_type} attention @ layer {layer_idx}, head {head_idx}, ({sample_type} {sample_idx + 1}/{num_samples})\"\n",
    "        )\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_map_from_spec(\n",
    "    model,\n",
    "    context: np.ndarray,\n",
    "    batch_idx: int,\n",
    "    layer_idx: int,\n",
    "    head_idx: int,\n",
    ") -> np.ndarray:\n",
    "    context_tensor = torch.from_numpy(context.T).float().to(model.device)[None, ...]\n",
    "    pred = model(context_tensor, output_attentions=True)\n",
    "    attn_weights = pred.attentions\n",
    "    attn = attn_weights[layer_idx][batch_idx, head_idx].detach().cpu().numpy()\n",
    "    return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this cell to roll a new random system\n",
    "test_data_dirs = \"/stor/work/AMDG_Gilpin_Summer2024/data/improved/final_skew40\"\n",
    "systems = glob(os.path.join(test_data_dirs, \"test_zeroshot/*\"))\n",
    "randsys = np.random.choice(systems).split(\"/\")[-1]\n",
    "print(randsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syspaths = get_system_filepaths(randsys, test_data_dirs, \"test_zeroshot\")\n",
    "trajectory, _ = load_trajectory_from_arrow(syspaths[0])\n",
    "\n",
    "sample_idx = 2\n",
    "layer_idx = 8  # even layers are temporal, odd are channel\n",
    "head_idx = 1\n",
    "start_time = 0\n",
    "context_length = 1024\n",
    "end_time = start_time + context_length\n",
    "attn_type = \"temporal\" if layer_idx % 2 == 0 else \"channel\"\n",
    "\n",
    "plot_attn_map(\n",
    "    pft_model.model,\n",
    "    trajectory[:, start_time:end_time],\n",
    "    sample_idx=sample_idx,\n",
    "    layer_idx=layer_idx,\n",
    "    head_idx=head_idx,\n",
    "    prefix=randsys,\n",
    "    colormap=\"Reds\",\n",
    "    show_title=False,\n",
    "    show_colorbar=False,\n",
    "    save_path=f\"{fig_dir}/{attn_type}-attn_{randsys}_layer-{layer_idx}_head-{head_idx}_sample-{sample_idx}_t0-{start_time}_clen-{context_length}.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_indices = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "attn_maps_by_layer = {}\n",
    "fft_attn_maps_by_layer = {}\n",
    "for layer_idx in layer_indices:\n",
    "    attn_map = get_attn_map_from_spec(\n",
    "        pft_model.model,\n",
    "        trajectory[:, start_time:end_time],\n",
    "        batch_idx=sample_idx,\n",
    "        layer_idx=layer_idx,\n",
    "        head_idx=head_idx,\n",
    "    )\n",
    "    fft_attn_maps_by_layer[layer_idx] = np.fft.fftshift(np.fft.fft2(attn_map))\n",
    "    attn_maps_by_layer[layer_idx] = attn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "n_rows = 2\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "\n",
    "axes = axes.flatten()\n",
    "for i, (layer_idx, attn_map) in enumerate(attn_maps_by_layer.items()):\n",
    "    axes[i].imshow(attn_map, cmap=\"Blues\")\n",
    "    axes[i].set_title(f\"Head {head_idx} - Layer {layer_idx}\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_3d_axes(ax_3d, scale: float = 0.8, elevation: float = 30, azimuth: float = 45):\n",
    "    \"\"\"Set up clean 3D axes with coordinate system from origin.\"\"\"\n",
    "    ax_3d.grid(False)\n",
    "    ax_3d.set_axis_off()\n",
    "\n",
    "    # Get data limits\n",
    "    xmin, xmax = ax_3d.get_xlim()\n",
    "    ymin, ymax = ax_3d.get_ylim()\n",
    "    zmin, zmax = ax_3d.get_zlim()\n",
    "\n",
    "    # Calculate origin and axis length\n",
    "    origin = [\n",
    "        min(0, xmin),\n",
    "        min(0, ymin),\n",
    "        min(0, zmin),\n",
    "    ]  # Ensure origin includes (0,0,0)\n",
    "    axis_length = scale * max(xmax - xmin, ymax - ymin, zmax - zmin)  # Slightly longer than data range\n",
    "\n",
    "    # Plot coordinate axes with thicker lines\n",
    "    ax_3d.plot(\n",
    "        [origin[0], origin[0] + axis_length],\n",
    "        [origin[1], origin[1]],\n",
    "        [origin[2], origin[2]],\n",
    "        \"k-\",\n",
    "        lw=1.5,\n",
    "    )  # x-axis\n",
    "    ax_3d.plot(\n",
    "        [origin[0]],\n",
    "        [origin[1], origin[1] + axis_length],\n",
    "        [origin[2], origin[2]],\n",
    "        \"k-\",\n",
    "        lw=1.5,\n",
    "    )  # y-axis\n",
    "    ax_3d.plot([origin[0]], [origin[1]], [origin[2], origin[2] + axis_length], \"k-\", lw=1.5)  # z-axis\n",
    "\n",
    "    # Add axis labels with better positioning and consistent style\n",
    "    label_offset = axis_length * 1.1\n",
    "    ax_3d.text(\n",
    "        origin[0] + label_offset,\n",
    "        origin[1],\n",
    "        origin[2],\n",
    "        \"$x_1$\",\n",
    "        fontsize=12,\n",
    "        ha=\"center\",\n",
    "    )\n",
    "    ax_3d.text(\n",
    "        origin[0],\n",
    "        origin[1] + label_offset,\n",
    "        origin[2],\n",
    "        \"$x_2$\",\n",
    "        fontsize=12,\n",
    "        ha=\"center\",\n",
    "    )\n",
    "    ax_3d.text(\n",
    "        origin[0],\n",
    "        origin[1],\n",
    "        origin[2] + label_offset,\n",
    "        \"$x_3$\",\n",
    "        fontsize=12,\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "    # Set better viewing angle\n",
    "    ax_3d.view_init(elev=elevation, azim=azimuth)  # Adjusted for better perspective\n",
    "\n",
    "    # Ensure axes limits include both data and coordinate system\n",
    "    margin = axis_length * 0.2\n",
    "    ax_3d.set_xlim(origin[0], origin[0] + axis_length + margin)\n",
    "    ax_3d.set_ylim(origin[1], origin[1] + axis_length + margin)\n",
    "    ax_3d.set_zlim(origin[2], origin[2] + axis_length + margin)\n",
    "\n",
    "\n",
    "def plot_model_prediction(\n",
    "    model,\n",
    "    context: np.ndarray,\n",
    "    groundtruth: np.ndarray,\n",
    "    prediction_length: int,\n",
    "    title: str | None = None,\n",
    "    save_path: str | None = None,\n",
    "    elevation: float = 30,\n",
    "    axis_scale: float = 0.6,\n",
    "    azimuth: float = 45,\n",
    "    linear_attn: bool = False,\n",
    "    **kwargs,\n",
    "):\n",
    "    context_tensor = torch.from_numpy(context.T).float().to(pft_model.device)[None, ...]\n",
    "    pred = model.predict(context_tensor, prediction_length, linear_attn=linear_attn).squeeze().cpu().numpy()\n",
    "    total_length = context.shape[1] + prediction_length\n",
    "    context_ts = np.arange(context.shape[1]) / total_length\n",
    "    pred_ts = np.arange(context.shape[1], total_length) / total_length\n",
    "\n",
    "    # Create figure with gridspec layout\n",
    "    fig = plt.figure(figsize=(6, 8))\n",
    "\n",
    "    # Create main grid with padding for colorbar\n",
    "    outer_grid = fig.add_gridspec(2, 1, height_ratios=[0.65, 0.35], hspace=-0.2)\n",
    "\n",
    "    # Create sub-grid for the plots\n",
    "    gs = outer_grid[1].subgridspec(3, 1, height_ratios=[0.2] * 3, wspace=0, hspace=0)\n",
    "    ax_3d = fig.add_subplot(outer_grid[0], projection=\"3d\")\n",
    "\n",
    "    ax_3d.plot(*context[:3], alpha=0.5, color=\"black\", label=\"Context\")\n",
    "    ax_3d.plot(*groundtruth[:3], linestyle=\"--\", color=\"black\", label=\"Groundtruth\")\n",
    "    ax_3d.plot(*pred.T[:3], color=\"red\", label=\"Prediction\")\n",
    "    ax_3d.set_xlabel(\"$x_1$\")\n",
    "    ax_3d.set_ylabel(\"$x_2$\")\n",
    "    ax_3d.set_zlabel(\"$x_3$\")  # type: ignore\n",
    "    setup_3d_axes(ax_3d, scale=axis_scale, elevation=elevation, azimuth=azimuth)\n",
    "\n",
    "    if title is not None:\n",
    "        title_name = title.replace(\"_\", \" \")\n",
    "        ax_3d.set_title(title_name, fontweight=\"bold\")\n",
    "\n",
    "    axes_1d = [fig.add_subplot(gs[i, 0]) for i in range(3)]\n",
    "    for i, ax in enumerate(axes_1d):\n",
    "        ax.plot(context_ts, context[i], alpha=0.5, color=\"black\")\n",
    "        ax.plot(pred_ts, groundtruth[i], linestyle=\"--\", color=\"black\")\n",
    "        ax.plot(pred_ts, pred[:, i], color=\"red\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_aspect(\"auto\")\n",
    "\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_attn_map(\n",
    "    model,\n",
    "    context: np.ndarray,\n",
    "    channel_i: int,\n",
    "    channel_j: int,\n",
    "    layer_idx: int,\n",
    "    head_idx: int | None = None,\n",
    "    colormap: str = \"magma\",\n",
    "    show_colorbar: bool = True,\n",
    "    save_path: str | None = None,\n",
    "    linear_attn: bool = False,\n",
    "    figsize: tuple[int, int] = (10, 10),\n",
    ") -> None:\n",
    "    \"\"\"Plot cross-attention matrix with corresponding timeseries patches along edges.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use for inference\n",
    "        context: Input context array\n",
    "        channel_i: Index of first channel\n",
    "        channel_j: Index of second channel\n",
    "        layer_idx: Layer index to visualize\n",
    "        head_idx: Head index to visualize. If None, plots all heads in a row\n",
    "        colormap: Colormap to use for attention visualization\n",
    "        show_colorbar: Whether to show colorbar\n",
    "        save_path: Path to save figure, if None displays instead\n",
    "        linear_attn: Whether to use linear attention\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    assert layer_idx in range(len(model.model.encoder.layers)), (\n",
    "        f\"Layer index must be in range of encoder layers (0-{len(model.model.encoder.layers)})\"\n",
    "    )\n",
    "    patch_size = model.config.patch_length\n",
    "    patches = context.reshape(context.shape[0], -1, patch_size)\n",
    "    n_patches = patches.shape[1]\n",
    "\n",
    "    context_tensor = torch.from_numpy(context.T).float().to(model.device)[None, ...]\n",
    "    pred = model(context_tensor, linear_attn=linear_attn, output_hidden_states=True)\n",
    "\n",
    "    hidden_state = pred.hidden_states[layer_idx]\n",
    "    hidden_state = hidden_state.view(-1, hidden_state.shape[-2], hidden_state.shape[-1])\n",
    "    layer = model.model.encoder.layers[layer_idx]\n",
    "    v = layer.temporal_self_attn.v_proj(hidden_state)\n",
    "    mixed_k = layer.channel_self_attn.k_proj(v)\n",
    "    mixed_q = layer.channel_self_attn.q_proj(v)\n",
    "    mixed_k = layer.channel_self_attn._shape(mixed_k, -1, mixed_k.shape[0])[channel_j]\n",
    "    mixed_q = layer.channel_self_attn._shape(mixed_q, -1, mixed_q.shape[0])[channel_i]\n",
    "    num_heads, seq_len, _ = mixed_k.shape\n",
    "    attn = (mixed_q @ mixed_k.transpose(1, 2)).reshape(num_heads, seq_len, seq_len)\n",
    "    attn = attn.detach().cpu().numpy()\n",
    "\n",
    "    if head_idx is None:\n",
    "        fig = plt.figure(figsize=(figsize[0] * num_heads, figsize[1]))\n",
    "        gs = fig.add_gridspec(1, num_heads + 1, width_ratios=[1] * num_heads + [0.05])\n",
    "\n",
    "        for h in range(num_heads):\n",
    "            ax = fig.add_subplot(gs[0, h])\n",
    "            vabs = np.abs(attn[h].max())\n",
    "            im = ax.imshow(\n",
    "                attn[h] / vabs,\n",
    "                extent=(0, n_patches, n_patches, 0),\n",
    "                cmap=colormap,\n",
    "                vmin=-1,\n",
    "                vmax=1,\n",
    "            )\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            if h == 0:\n",
    "                ax.set_ylabel(\"Channel j\")\n",
    "            ax.set_title(f\"Head {h}\")\n",
    "\n",
    "        if show_colorbar:\n",
    "            ax_cbar = fig.add_subplot(gs[0, -1])\n",
    "            plt.colorbar(im, cax=ax_cbar)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        outer_grid = fig.add_gridspec(1, 2, width_ratios=[1, 0.05], wspace=0.05)\n",
    "        gs = outer_grid[0].subgridspec(\n",
    "            2,\n",
    "            2,\n",
    "            width_ratios=[0.15, 0.85],\n",
    "            height_ratios=[0.15, 0.85],\n",
    "            wspace=0,\n",
    "            hspace=0,\n",
    "        )\n",
    "        ax_main = fig.add_subplot(gs[1, 1])\n",
    "        vabs = np.abs(attn[head_idx].max())\n",
    "        im = ax_main.imshow(\n",
    "            attn[head_idx] / vabs,\n",
    "            extent=(0, n_patches, n_patches, 0),\n",
    "            cmap=colormap,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "        )\n",
    "        ax_main.set_xticks([])\n",
    "        ax_main.set_yticks([])\n",
    "\n",
    "        # Plot patches along top\n",
    "        ax_top = fig.add_subplot(gs[0, 1])\n",
    "        for i in range(n_patches):\n",
    "            x = np.linspace(i, i + 1, patch_size)\n",
    "            ax_top.plot(x, patches[channel_j, i], linewidth=1)\n",
    "        ax_top.set_xlim(0, n_patches)\n",
    "        ax_top.set_xticks([])\n",
    "        ax_top.set_yticks([])\n",
    "        ax_top.grid(True)\n",
    "\n",
    "        # Plot patches along left side\n",
    "        ax_left = fig.add_subplot(gs[1, 0])\n",
    "        for i in range(n_patches):\n",
    "            y = np.linspace(i, i + 1, patch_size)\n",
    "            ax_left.plot(-patches[channel_i, i], y, linewidth=1)\n",
    "        ax_left.set_ylim(n_patches, 0)\n",
    "        ax_left.set_xticks([])\n",
    "        ax_left.set_yticks([])\n",
    "        ax_left.grid(True)\n",
    "\n",
    "        ax_cbar = fig.add_subplot(outer_grid[1])\n",
    "        if show_colorbar:\n",
    "            plt.colorbar(im, cax=ax_cbar)\n",
    "        else:\n",
    "            fig.delaxes(ax_cbar)\n",
    "\n",
    "        fig.delaxes(fig.add_subplot(gs[0, 0]))\n",
    "\n",
    "        # Force exact alignment of subplots\n",
    "        main_pos = ax_main.get_position()\n",
    "        ax_top.set_position(\n",
    "            [main_pos.x0, main_pos.y1, main_pos.width, ax_top.get_position().height]  # type: ignore\n",
    "        )\n",
    "        ax_left.set_position(\n",
    "            [\n",
    "                ax_left.get_position().x0,\n",
    "                main_pos.y0,\n",
    "                ax_left.get_position().width,\n",
    "                main_pos.height,\n",
    "            ]  # type: ignore\n",
    "        )\n",
    "        ax_cbar.set_position(\n",
    "            [\n",
    "                ax_cbar.get_position().x0,\n",
    "                main_pos.y0,\n",
    "                ax_cbar.get_position().width,\n",
    "                main_pos.height,\n",
    "            ]  # type: ignore\n",
    "        )\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this cell to roll a new random system\n",
    "test_data_dirs = \"/stor/work/AMDG_Gilpin_Summer2024/data/final_skew40\"\n",
    "systems = glob(os.path.join(test_data_dirs, \"test_zeroshot/*\"))\n",
    "randsys = np.random.choice(systems).split(\"/\")[-1]\n",
    "print(randsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syspaths = get_system_filepaths(randsys, test_data_dirs, \"test_zeroshot\")\n",
    "trajectory, _ = load_trajectory_from_arrow(syspaths[0])\n",
    "\n",
    "layer_idx = 3\n",
    "head_idx = None\n",
    "start_time = 64\n",
    "context_length = 1024\n",
    "end_time = start_time + context_length\n",
    "\n",
    "# preview the heads first\n",
    "plot_cross_attn_map(\n",
    "    pft_model.model,\n",
    "    trajectory[:, start_time:end_time],\n",
    "    channel_i=0,\n",
    "    channel_j=1,\n",
    "    layer_idx=layer_idx,\n",
    "    head_idx=head_idx,\n",
    "    colormap=\"Reds\",\n",
    "    show_colorbar=False,\n",
    "    # save_path=f\"../figures/crossattn_{randsys}_layer-{layer_idx}_head-{head_idx}_t0-{start_time}_clen-{context_length}.pdf\",\n",
    "    figsize=(3, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_idx = 0\n",
    "plot_cross_attn_map(\n",
    "    pft_model.model,\n",
    "    trajectory[:, start_time:end_time],\n",
    "    channel_i=0,\n",
    "    channel_j=1,\n",
    "    layer_idx=layer_idx,\n",
    "    head_idx=head_idx,\n",
    "    colormap=\"Reds\",\n",
    "    show_colorbar=False,\n",
    "    save_path=f\"../figures/crossattn_{randsys}_layer-{layer_idx}_head-{head_idx}_t0-{start_time}_clen-{context_length}.pdf\",\n",
    "    figsize=(3, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
