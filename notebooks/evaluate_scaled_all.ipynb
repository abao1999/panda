{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from panda.utils.plot_utils import apply_custom_style, make_box_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_custom_style(\"../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = os.path.join(\"../figs\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_split = \"final_skew40/test_zeroshot\"\n",
    "data_split = \"test_zeroshot\"\n",
    "\n",
    "run_names_panda = {\n",
    "    # \"Panda 72M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"patchtst\",\n",
    "    #     \"panda_nh12_dmodel768_mixedp-4\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Panda 42M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"patchtst\",\n",
    "    #     \"panda_nh10_dmodel640-1\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Panda 21M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"patchtst\",\n",
    "    #     \"polyembed_21M_iter400k_dataimproved-2\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    \"Panda 21M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"patchtst\",\n",
    "        \"pft_chattn_emb_w_poly-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "}\n",
    "\n",
    "run_names_chronos_zs = {\n",
    "    # \"Chronos 20M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"chronos\",\n",
    "    #     \"chronos_mini_zeroshot\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Chronos 46M\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     # \"chronos_nondeterministic\",\n",
    "    #     \"chronos\",\n",
    "    #     \"chronos_small_zeroshot\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    \"Chronos 200M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        # \"chronos\",\n",
    "        \"chronos_nondeterministic\",\n",
    "        \"chronos_base_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "}\n",
    "\n",
    "run_names_chronos_sft = {\n",
    "    # \"Chronos 46M SFT\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"chronos\",\n",
    "    #     # \"chronos_nondeterministic\",\n",
    "    #     \"chronos_small_ft-4\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    \"Chronos 20M SFT\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        # \"chronos\",\n",
    "        \"chronos_nondeterministic\",\n",
    "        \"chronos_t5_mini_ft-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "}\n",
    "\n",
    "run_metrics_dirs_all_groups = {\n",
    "    \"panda\": run_names_panda,\n",
    "    \"chronos_zs\": run_names_chronos_zs,\n",
    "    \"chronos_sft\": run_names_chronos_sft,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dirs_all_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = defaultdict(lambda: defaultdict(dict))\n",
    "for run_group, run_metrics_dir_dict in run_metrics_dirs_all_groups.items():\n",
    "    print(f\"Run group: {run_group}\")\n",
    "    for run_abbrv, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "        print(run_abbrv)\n",
    "        if not os.path.exists(run_metrics_dir):\n",
    "            continue\n",
    "        run_abbrv = str(run_abbrv)\n",
    "        print(f\"{run_abbrv}: {run_metrics_dir}\")\n",
    "        csv_files = [file for file in os.listdir(run_metrics_dir) if file.endswith(\".csv\")]\n",
    "        for file in sorted(\n",
    "            csv_files,\n",
    "            key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "        ):\n",
    "            prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "            # print(f\"Prediction length: {prediction_length} for {run_abbrv}\")\n",
    "            with open(os.path.join(run_metrics_dir, file)) as f:\n",
    "                metrics = pd.read_csv(f).to_dict()\n",
    "                metrics_all[run_group][run_abbrv][prediction_length] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_groups = defaultdict(lambda: defaultdict(dict))\n",
    "for run_group, all_metrics_of_run_group in metrics_all.items():\n",
    "    print(run_group)\n",
    "    for run_abbrv, all_metrics_of_run_abbrv in all_metrics_of_run_group.items():\n",
    "        print(run_abbrv)\n",
    "        for run_name, metrics in all_metrics_of_run_abbrv.items():\n",
    "            print(run_name)\n",
    "            systems = metrics.pop(\"system\")\n",
    "            metrics_unrolled = {k: list(v.values()) for k, v in metrics.items()}\n",
    "            print(metrics_unrolled.keys())\n",
    "            unrolled_metrics_all_groups[run_group][run_abbrv][run_name] = metrics_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_combined = {\n",
    "    **unrolled_metrics_all_groups[\"panda\"],\n",
    "    **unrolled_metrics_all_groups[\"chronos_zs\"],\n",
    "    **unrolled_metrics_all_groups[\"chronos_sft\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_metrics_dict(unrolled_metrics, metric_name):\n",
    "    summary_metrics_dict = defaultdict(dict)\n",
    "    for model_name, metrics_dict in unrolled_metrics.items():\n",
    "        print(model_name)\n",
    "        prediction_lengths = list(metrics_dict.keys())\n",
    "        summary_metrics_dict[model_name][\"prediction_lengths\"] = prediction_lengths\n",
    "        means = []\n",
    "        medians = []\n",
    "        stds = []\n",
    "        for prediction_length in prediction_lengths:\n",
    "            metric_val = metrics_dict[prediction_length][metric_name]\n",
    "            means.append(np.nanmean(metric_val))\n",
    "            medians.append(np.nanmedian(metric_val))\n",
    "            stds.append(np.nanstd(metric_val))\n",
    "        summary_metrics_dict[model_name][\"means\"] = means\n",
    "        summary_metrics_dict[model_name][\"medians\"] = medians\n",
    "        summary_metrics_dict[model_name][\"stds\"] = stds\n",
    "    return summary_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_prediction_length(metrics_dict, metric_name, show_std_envelope=False):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    for model_name, metrics in metrics_dict.items():\n",
    "        plt.plot(\n",
    "            metrics[\"prediction_lengths\"],\n",
    "            metrics[\"medians\"],\n",
    "            marker=\"o\",\n",
    "            label=model_name,\n",
    "        )\n",
    "        std_envelope = np.array(metrics[\"stds\"])\n",
    "        if show_std_envelope:\n",
    "            plt.fill_between(\n",
    "                metrics[\"prediction_lengths\"],\n",
    "                metrics[\"means\"] - std_envelope,\n",
    "                metrics[\"means\"] + std_envelope,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"Prediction Length\")\n",
    "    plt.title(metric_name, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dirs_all_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names_chosen = [\n",
    "    # \"mse\",\n",
    "    \"mae\",\n",
    "    \"smape\",\n",
    "    \"spearman\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict = defaultdict(dict)\n",
    "\n",
    "for run_group in run_metrics_dirs_all_groups.keys():\n",
    "    all_metrics_dict[run_group] = {\n",
    "        metrics_name: get_summary_metrics_dict(unrolled_metrics_all_groups[run_group], metrics_name)\n",
    "        for metrics_name in metric_names_chosen\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = plt.cm.tab10.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_combined.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = len(unrolled_metrics_all_combined)\n",
    "print(n_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs_panda = len(run_names_panda)\n",
    "n_runs_chronos_zs = len(run_names_chronos_zs)\n",
    "n_runs_chronos_sft = len(run_names_chronos_sft)\n",
    "\n",
    "# panda_colors = [plt.cm.tab20.colors[6], plt.cm.tab20.colors[7]]\n",
    "# chronos_zs_colors = [plt.cm.tab20.colors[8], plt.cm.tab20.colors[9]]\n",
    "# chronos_sft_colors = [plt.cm.tab20.colors[0]]\n",
    "panda_colors = [plt.cm.tab20.colors[6]]\n",
    "chronos_zs_colors = [plt.cm.tab20.colors[8]]\n",
    "chronos_sft_colors = [plt.cm.tab20.colors[0]]\n",
    "\n",
    "bar_colors = panda_colors + chronos_sft_colors + chronos_zs_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pred_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles = make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=selected_pred_length,\n",
    "    metric_to_plot=\"smape\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    save_path=f\"scaled_figs/smape_{selected_pred_length}.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(40, 60),\n",
    "    whisker_percentile_range=(25, 75),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2.5, 5)},\n",
    "    box_width=0.8,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(4, 0.6))\n",
    "plt.figure(figsize=(5, 1))\n",
    "\n",
    "# Add the legend\n",
    "plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"center\",\n",
    "    frameon=True,\n",
    "    ncol=5,\n",
    "    framealpha=1.0,\n",
    "    fontsize=12,\n",
    ")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"scaled_figs/ablations_legend_horizontal.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pred_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=selected_pred_length,\n",
    "    metric_to_plot=\"spearman\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"scaled_figs/spearman_{selected_pred_length}.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(40, 60),\n",
    "    whisker_percentile_range=(25, 75),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2.5, 5)},\n",
    "    box_width=0.8,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"mae\"\n",
    "\n",
    "make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=selected_pred_length,\n",
    "    metric_to_plot=metric_name,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"scaled_figs/{metric_name}_{selected_pred_length}.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"upper left\",\n",
    "        \"frameon\": True,\n",
    "        \"ncol\": 1,\n",
    "        \"framealpha\": 0.8,\n",
    "        # \"prop\": {\"weight\": \"bold\", \"size\": 5},\n",
    "        \"prop\": {\"size\": 6.8},\n",
    "    },\n",
    "    box_percentile_range=(40, 60),\n",
    "    whisker_percentile_range=(25, 75),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2.5, 5)},\n",
    "    box_width=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
